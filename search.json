[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Operating Systems and Networks SoSe 25 Notes",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "process.html",
    "href": "process.html",
    "title": "1  Process Management",
    "section": "",
    "text": "Condition Variables and Producer / Consumer Problem\nCondition variables are employed together with mutexes when synchronizing producers and consumers. It woul be incorrect ot only use a condition variable without a mutex, or a mutex with busy waiting without a condition varible.",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "process.html#condition-variables-and-producer-consumer-problem",
    "href": "process.html#condition-variables-and-producer-consumer-problem",
    "title": "1  Process Management",
    "section": "",
    "text": "Incorrect Variant 1: Condition Variable Without Mutex\nready = False\ncondition = ConditionVariable()\n\ndef wait_thread():\n    if not ready:\n        condition.wait()  # Incorrect: no mutex guarding shared state\n    print(\"Condition met!\")\n\ndef signal_thread():\n    ready = True\n    condition.notify()\nWhy It’s Wrong:\n\nAccess to ready is unprotected — race conditions may occur.\ncondition.wait() must always be used with a mutex.\n\n\n\nIncorrect Variant 2: Mutex Without Condition Variable (Busy Waiting)\nready = False\nmutex = Mutex()\n\ndef wait_thread():\n    while True:\n        mutex.lock()\n        if ready:\n            mutex.unlock()\n            break\n        mutex.unlock()\n        sleep(0.01)  # Active polling (wasteful)\n\ndef signal_thread():\n    mutex.lock()\n    ready = True\n    mutex.unlock()\nWhy It’s Problematic:\n\nAvoids races, but wastes CPU via busy waiting.\nAlso prone to subtle visibility issues if memory barriers aren’t enforced.\n\n\n\nCorrect Variant: Condition Variable with Mutex\nready = False\nmutex = Mutex()\ncondition = ConditionVariable()\n\ndef wait_thread():\n    mutex.lock()\n    while not ready:\n        condition.wait(mutex)  # Atomically unlocks and waits\n    mutex.unlock()\n    print(\"Condition met!\")\n\ndef signal_thread():\n    mutex.lock()\n    ready = True\n    condition.notify()\n    mutex.unlock()\nWhy It Works:\n\nShared state is properly guarded.\nNo busy waiting.\nSafe signaling and waking.\n\nAnother question is why to use while not ready and not simply if not ready:\ndef wait_thread():\n    mutex.lock()\n    if not ready:\n        condition.wait(mutex)\n    mutex.unlock()\nProblem:\n\nMay miss spurious wakeups or situations where multiple threads wait and only one should proceed.\nA while loop is necessary to recheck the condition after being woken up.\n\n\n\n\nProducer/Consumer Problem\n\nVariant A: Unbounded Queue (No Buffer Limit)\nqueue = []\nmutex = Mutex()\nnot_empty = ConditionVariable()\n\ndef producer():\n    while True:\n        item = produce()\n        mutex.lock()\n        queue.append(item)\n        not_empty.notify()\n        mutex.unlock()\n\ndef consumer():\n    while True:\n        mutex.lock()\n        while not queue:\n            not_empty.wait(mutex)\n        item = queue.pop(0)\n        mutex.unlock()\n        consume(item)\n\n\n\nVariant B: Bounded Queue (Fixed Buffer Size)\nqueue = []\nBUFFER_SIZE = 10\nmutex = Mutex()\nnot_empty = ConditionVariable()\nnot_full = ConditionVariable()\n\ndef producer():\n    while True:\n        item = produce()\n        mutex.lock()\n        while len(queue) &gt;= BUFFER_SIZE:\n            not_full.wait(mutex)\n        queue.append(item)\n        not_empty.notify()\n        mutex.unlock()\n\ndef consumer():\n    while True:\n        mutex.lock()\n        while not queue:\n            not_empty.wait(mutex)\n        item = queue.pop(0)\n        not_full.notify()\n        mutex.unlock()\n        consume(item)",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "process.html#summary-table",
    "href": "process.html#summary-table",
    "title": "1  Process Management",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\n\n\nCase\nUses Mutex\nUses Condition Variable\nBlocking\nCPU-Efficient\nCorrect\n\n\n\n\n1. Condition variable without mutex\nNo\nYes\nNo\nYes\nNo\n\n\n2. Mutex without condition variable\nYes\nNo\nNo\nNo (busy)\nPartly\n\n\n3. Condition variable with mutex\nYes\nYes\nYes\nYes\nYes\n\n\n4. If instead of while\nYes\nYes\nYes\nYes\nRisky\n\n\n5. Producer/Consumer (unbounded)\nYes\nYes (not_empty)\nYes\nYes\nYes\n\n\n6. Producer/Consumer (bounded)\nYes\nYes (not_empty, not_full)\nYes\nYes\nYes\n\n\n\n\n\nOperations of a Bounded Queue\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nin\nout\nBuffer State\nCount == ((in - out + 5) % 5)\n\n\n\n\n0\nStart\n0\n0\n[_ _ _ _ _]\n0\n\n\n1\nProduce A\n1\n0\n[A _ _ _ _]\n1\n\n\n2\nProduce B\n2\n0\n[A B _ _ _]\n2\n\n\n3\nProduce C\n3\n0\n[A B C _ _]\n3\n\n\n4\nConsume → A\n3\n1\n[_ B C _ _]\n2\n\n\n5\nConsume → B\n3\n2\n[_ _ C _ _]\n1\n\n\n6\nProduce D\n4\n2\n[_ _ C D _]\n2\n\n\n7\nProduce E\n0\n2\n[_ _ C D E]\n3\n\n\n8\nConsume → C\n0\n3\n[_ _ _ D E]\n2\n\n\n9\nProduce F\n1\n3\n[F _ _ D E]\n3\n\n\n\nwhere\n\nin: the write position / index\nout: the read position /index\ncount == (in - out + 5) % 5 is the invariant of the data structure, giving the number of elements in the buffer",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "memory.html",
    "href": "memory.html",
    "title": "2  Memory Management",
    "section": "",
    "text": "Virtual Memory",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Memory Management</span>"
    ]
  },
  {
    "objectID": "memory.html#virtual-memory",
    "href": "memory.html#virtual-memory",
    "title": "2  Memory Management",
    "section": "",
    "text": "Paging\n\nTranslating Logical to Physical Addresses\n\nContext\nIn paging, the operating system divides:\n\nLogical (virtual) memory into fixed-size pages\nPhysical memory (RAM) into same-size frames\n\nEach process has a page table that maps page numbers to frame numbers.\nOur goal is:\n\nGiven a virtual address, compute the corresponding physical address.\n\n\n\nExample Setup\n\nVirtual address \\(V = 7000\\)\nPage size = 4096 bytes = \\(2^{12}\\) ⇒ \\(k = 12\\)\nAssume the page table maps page 1 to frame 9: \\(F(1) = 9\\)\n\n\n\nStep 1: Manual (Arithmetic) Calculation\nTo translate a virtual address manually, we need to answer two questions:\n\nWhich page is the address in?\nWhere within that page is the address?\n\nThis is done by:\n\nDividing the address by the page size to get the page number\nTaking the remainder (modulo) to get the offset within the page\n\nApply this to \\(V = 7000\\) with page size 4096:\n\nPage number \\(p = \\left\\lfloor \\frac{7000}{4096} \\right\\rfloor = 1\\)\nOffset \\(d = 7000 \\mod 4096 = 2904\\)\n\nNow we look up page 1 in the page table:\n\nFrame number \\(f = F(1) = 9\\)\n\nTo get the final physical address, we compute the base address of frame 9 and add the offset:\n\nPhysical address = \\(f \\cdot 4096 + d = 9 \\cdot 4096 + 2904 = 39768\\)\n\nResult: 39768\n\n\nStep 2: Bitwise Calculation (Optimized for Hardware)\nFor power-of-two page sizes, the address can be efficiently split using bitwise operations:\n\nPage number = \\(V \\gg 12\\) (right shift by 12 bits is equivalent to dividing by 4096)\nOffset = \\(V \\& (2^{12} - 1) = V \\& 0xFFF\\) (bit mask keeps the lower 12 bits)\nPage number 1 maps to frame number \\(f = F(1) = 9\\)\n\nTo compute the frame’s starting address, we use a left shift:\n\n\\(f \\ll 12 = 9 \\ll 12 = 36864\\), which is equivalent to \\(9 \\cdot 4096\\)\n\nFinal physical address:\n\n\\(\\text{Physical Address} = 36864 + 2904 = 39768\\)\n\nSame result, now using fast bit operations.\n\n\nBit Sequence Visualization\nLet’s visualize how the virtual address is split in binary:\n\nVirtual address \\(V = 7000\\)\nBinary representation (14 bits): 0001 1011 0101 1000\n\nSplit into:\n\nPage number (upper 2 bits): 00 01 → 1\nOffset (lower 12 bits): 1011 0101 1000 → 2904\n\nThis split works because:\n\nThe lower 12 bits represent the offset for a 4 KB page\nThe upper bits index into the page table\n\n\n\nWhy This Works Mathematically\nThe logic behind using bit shifts and masks instead of division and modulo is based on how numbers are represented in binary.\n\nDecimal Analogy (Base 10)\nConsider dividing 1375 by powers of 10:\n\n1375 ÷ 10¹ = 137 (modulo: 5)\n1375 ÷ 10² = 13 (modulo: 75)\n1375 ÷ 10³ = 1 (modulo: 375)\n\nThe rightmost digits are the remainder (modulo); the left are the quotient (division).\n\n\nBinary Example (Base 2)\nTake the binary number 1011₂ (= 11₁₀):\n\n1011 ÷ 2¹ = 101 = 5 (modulo: 1)\n1011 ÷ 2² = 10 = 2 (modulo: 11 = 3)\n1011 ÷ 2³ = 1 = 1 (modulo: 011 = 3)\n\nIn both systems, the rightmost digits/bits represent the offset, and the leftmost represent the page number.\nThis is why in binary:\n\n\\(V \\gg k\\) is equivalent to \\(\\left\\lfloor V / 2^k \\right\\rfloor\\)\n\\(V \\& (2^k - 1)\\) is equivalent to \\(V \\mod 2^k\\)\n\\(f \\ll k\\) is equivalent to \\(f \\cdot 2^k\\), which gives the frame base address\n\nThese operations are both mathematically correct and hardware-efficient.\n\n\n\nFinal Formula\n\\[\n\\text{Physical Address} = \\left( F(V \\gg k) \\ll k \\right) + \\left( V \\& (2^k - 1) \\right)\n\\]\nThis computes:\n\nThe page number via right shift\nThe frame number from the page table\nThe frame base via left shift (i.e., multiplying by page size)\nThe final physical address by adding the offset\n\n\n\nAdditional Example for Practice and Clarity\nLet’s now take another address and apply all three methods for reinforcement.\n\nSetup\n\nVirtual address \\(V = 13,\\!452\\)\nPage size = 4096 = \\(2^{12}\\)\nPage table:\n\n\n\n\nPage #\nFrame #\n\n\n\n\n0\n3\n\n\n1\n7\n\n\n2\n1\n\n\n3\n6\n\n\n\n\n\nManual Calculation\n\nPage number: \\(13,\\!452 \\div 4096 = 3\\)\nOffset: \\(13,\\!452 \\mod 4096 = 1164\\)\nFrame number: \\(F(3) = 6\\)\nPhysical address = \\(6 \\cdot 4096 + 1164 = 24,\\!576 + 1164 = 25,\\!740\\)\n\n\n\nBitwise Calculation\n\n\\(V = 13,\\!452 = 0b0011\\ 0100\\ 1001\\ 1100\\)\nPage number = \\(V \\gg 12 = 3\\)\nOffset = \\(V \\& 0xFFF = 1164\\)\nFrame number = \\(F(3) = 6\\)\nFrame base = \\(6 \\ll 12 = 24,\\!576\\)\nPhysical address = \\(24,\\!576 + 1164 = 25,\\!740\\)\n\n\n\n\nUsing the Formula\n\\[\n\\text{Physical Address} = (F(V \\gg 12) \\ll 12) + (V \\& 0xFFF)\n\\]\n\\[\n= (6 \\ll 12) + 1164 = 24,\\!576 + 1164 = 25,\\!740\n\\]\n\n\nConclusion\nWhen the page size is a power of two, address translation can be performed using fast bit operations instead of division and modulo. This is possible because of how binary numbers encode positional value. We saw that the lower bits give the offset and the upper bits the page number. Whether done manually, with bit operations, or using the translation formula, all approaches yield the same physical address — and this consistency is what makes paging both robust and efficient.\nAbsolutely — here is the complete regenerated summary, now incorporating:\n\nThe updated “Inverted Page Tables” section with size explanation and example\nThe updated “Hierarchical Page Tables” section with both the 32-bit and 64-bit address resolution examples and definitions of each table level\nConsistent formatting throughout:\n\nNo bold in headers\nMinimal boldface emphasis in the text — used only where strictly useful for clarity\n\n\nThis version is fully ready for integration into your Quarto notes.\n\n\n\n\nPage Tables\n\nSingle-Level (Direct) Page Tables\nIn the simplest form of paging, each process has its own single-level page table, which directly maps virtual page numbers to physical frame numbers.\nFor example, in a system with:\n\nA 32-bit virtual address space (4 GB total)\nA page size of 4 KB = \\(2^{12}\\) bytes\n\nThe number of virtual pages is:\n\\[\n2^{32} / 2^{12} = 2^{20} = 1,\\!048,\\!576 \\text{ entries}\n\\]\nIf each page table entry (PTE) is 4 bytes, the total size of the page table is:\n\\[\n2^{20} \\times 4 = 4 \\text{ MB per process}\n\\]\nIn a 64-bit system, even with larger pages (e.g. 4 MB), the number of virtual pages is so large (e.g., \\(2^{52}\\)) that flat page tables become completely impractical.\n\n\n\nWhy Single-Level Tables Are Impractical\nMain issues:\n\nMemory usage per process becomes excessive (e.g., 4 MB/page table × hundreds of processes)\nScaling issues as address spaces grow\nMost processes use only a small part of their virtual address space, so allocating full page tables is wasteful\n\nThus, alternative paging strategies are needed.\n\n\n\nFrame Table (Global Physical Memory Tracking)\nThe OS maintains a global frame table, which tracks:\n\nWhich physical frames are in use\nWhat each frame is used for (user page, kernel structure, page table, etc.)\nAssociated metadata: dirty bit, reference count, owner process\n\nThis allows the OS to allocate and deallocate physical memory intelligently and safely. It is also crucial for page replacement algorithms, memory protection, and managing shared pages or I/O buffers.\n\n\n\nInverted Page Tables\nIn a traditional page table system, each process maintains its own page table, which maps virtual pages to physical frames. In contrast, an inverted page table uses a fundamentally different approach:\n\nThere is a single global page table for the entire system\nIt contains one entry for each physical frame, not for each virtual page\nEach entry records:\n\nThe process ID that owns the frame\nThe virtual page number that maps to it\nAny additional metadata (e.g., access flags, validity)\n\n\nThis approach dramatically reduces memory overhead, especially in systems with large virtual address spaces.\n\nSize of the inverted page table\nThe size of an inverted page table depends only on the number of physical frames, which is determined by:\n\\[\n\\text{Number of entries} = \\frac{\\text{RAM size}}{\\text{frame size}}\n\\]\nEach entry stores fixed-size metadata (such as PID and VPN), so the total size is:\n\\[\n\\text{Table size} = \\frac{\\text{RAM size}}{\\text{frame size}} \\times \\text{entry size}\n\\]\nThe ratio of the table size to RAM size simplifies to:\n\\[\n\\frac{\\text{Table size}}{\\text{RAM size}} = \\frac{\\text{entry size}}{\\text{frame size}}\n\\]\nThis ratio is independent of total RAM size. In other words, the memory overhead of the page table scales proportionally with RAM but is bounded by the frame size and the entry size.\n\n\nConcrete example\nConsider a 32-bit system with the following properties:\n\nPhysical RAM: 4 GB = \\(2^{32}\\) bytes\nPage/frame size: 4 KB = \\(2^{12}\\) bytes\nPage table entry size: 8 bytes (to store PID, VPN, flags, etc.)\n\nStep-by-step:\n\nNumber of physical frames:\n\n\\[\n\\frac{2^{32}}{2^{12}} = 2^{20} = 1,\\!048,\\!576 \\text{ frames}\n\\]\n\nTotal inverted page table size:\n\n\\[\n2^{20} \\times 8 = 8 \\text{ MB}\n\\]\n\nRelative overhead:\n\n\\[\n\\frac{8 \\text{ MB}}{4 \\text{ GB}} = \\frac{1}{512}\n\\]\nThis means the page table occupies only about 0.2% of RAM.\n\n\nSummary of trade-offs\nInverted page tables offer substantial memory savings, especially on systems with large or sparsely used virtual address spaces. However, the downside is that address translation becomes more complex:\n\nThe system must search (or hash) the page table to find the matching (process ID, virtual page) pair\nThis lookup is slower than direct indexing\nTLB caching becomes less straightforward\n\nFor this reason, inverted page tables are rarely used in modern general-purpose OSes, though they are still valuable in embedded or resource-constrained systems.\n\n\n\n\nHierarchical Page Tables\nModern systems (e.g., x86, Linux, Windows) use multi-level page tables to avoid allocating massive single-level tables for sparse address spaces. The key idea is to divide the virtual address into multiple segments, each of which indexes a level in the page table hierarchy. This allows the OS to only allocate memory for regions that are actually used.\nEach level of the hierarchy resolves part of the virtual address and points to the next level down. The final level contains the physical frame number. The remaining bits (the offset) are added to form the final physical address.\nThis approach reduces memory overhead and supports sparse, large virtual address spaces.\n\n32-bit Two-Level Paging Example\nAssume a 32-bit virtual address space with:\n\nPage size = 4 KB = \\(2^{12}\\)\n10 bits for the page directory index\n10 bits for the page table index\n12 bits for the offset\n\nThe virtual address layout is:\n[ 10 bits | 10 bits | 12 bits ]\n   PD index  PT index   Offset\nSuppose the virtual address is:\nVA = 0x1234ABCD\nConvert to binary:\n0001 0010 0011 0100 1010 1011 1100 1101\nSplit:\n\nPage Directory index = 0001001000 = 0x048 = 72\nPage Table index = 1101001010 = 0x34A = 842\nOffset = 101111001101 = 0xBCD = 3021\n\nAssume:\n\nThe page directory is located at physical address 0x00100000\nEntry 72 in the page directory points to a page table at 0x00200000\nEntry 842 in that page table points to a physical frame at 0x00ABC000\n\nFinal physical address:\n\\[\n0x00ABC000 + 0xBCD = 0x00ABCBCD\n\\]\nThis example illustrates how a 2-level table hierarchy resolves the virtual address to a physical address through two indirections and an offset.\n\n\n64-bit Four-Level Paging Example\nModern x86-64 systems typically support a 48-bit virtual address space, split across four paging levels. The page size remains 4 KB = \\(2^{12}\\).\nEach level of the hierarchy resolves 9 bits (since \\(2^9 = 512\\) entries per table), so the full 48-bit address is broken into:\n[ 9 bits | 9 bits | 9 bits | 9 bits | 12 bits ]\n  PML4     PDPT     PD       PT       Offset\nThe levels are defined as follows:\n\nPML4 (Page Map Level 4): The root of the page table hierarchy; indexed by the top 9 bits of the virtual address. Each entry points to a PDPT.\nPDPT (Page Directory Pointer Table): Intermediate level; each entry points to a Page Directory.\nPD (Page Directory): Each entry points to a Page Table.\nPT (Page Table): Final level; each entry contains a physical frame number.\nOffset: Specifies the exact byte within the 4 KB page.\n\nSuppose the virtual address is:\nVA = 0x00007F34_1234ABCD\nBreaking down the lower 48 bits:\n\nPML4 index = bits 47–39 = 0\nPDPT index = bits 38–30 = 505\nPD index = bits 29–21 = 322\nPT index = bits 20–12 = 210\nOffset = bits 11–0 = 0xAF3D = 44861\n\nAssume the following physical mappings:\n\nCR3 register points to PML4 at 0x00100000\nPML4 entry 0 → PDPT at 0x00200000\nPDPT entry 505 → PD at 0x00300000\nPD entry 322 → PT at 0x00400000\nPT entry 210 → frame at 0x00ABC000\n\nFinal physical address:\n\\[\n0x00ABC000 + 0xAF3D = 0x00B06F3D\n\\]\nThis example demonstrates how a virtual address is translated step-by-step through four levels of indirection. The layered structure supports extremely large address spaces (up to 256 TB) without requiring full allocation of all intermediate tables.\n\n\nSummary\nHierarchical page tables solve the scalability problem of flat page tables by breaking the virtual address into multiple segments. Each level of the hierarchy is a smaller table, and lower levels are allocated only when needed. This provides a sparse, memory-efficient structure for address translation.\nThe cost of additional indirection is mitigated by the use of TLBs, which cache recent address translations to avoid repeated page walks.\n\n\n\n\nWhen and How Page Tables Are Allocated\nPage tables are allocated in two situations:\n\nAt program load time The OS allocates top-level tables and reserves virtual address regions for code, data, stack, etc., but not necessarily all intermediate tables.\nOn demand via page faults When a process accesses a virtual address with no current mapping, the CPU triggers a page fault. If the access is valid, the OS allocates missing intermediate page tables and a physical frame, updates the page table entries, and resumes execution.\n\nThis approach enables sparse memory allocation and efficient use of physical memory.\n\n\n\nPhysical Memory: Frames and Their Usage\nRAM is divided into fixed-size frames (e.g., 4 KB). Each frame can hold:\n\nA user page (code, stack, heap)\nA page table (of any level)\nA kernel structure\nOther memory-resident objects\n\nContiguous physical frames may contain completely unrelated contents, as physical memory management is modular and page-based. The OS tracks frame usage via the global frame table.\n\n\n\nKernel Mapping and Access\nThe kernel is mapped into the upper region of each process’s virtual address space (e.g., from 0xC0000000 upward in 32-bit systems). This allows:\n\nFast system calls and interrupt handling\nAvoiding page table switches on mode transitions\n\nThis region is protected by page table flags, preventing access in user mode. The kernel itself runs entirely in kernel mode. Its physical location is determined at boot and may vary across systems. Techniques like KASLR (Kernel Address Space Layout Randomization) add further protection.\n\n\n\nTranslation Lookaside Buffer\nThe TLB is a hardware-managed cache used by the MMU to store recently used virtual-to-physical page translations.\nWhy it’s important:\n\nPage walks involve multiple memory accesses\nThe TLB allows near-instant translation on a hit\nReduces the average cost of memory access in the presence of multi-level page tables\n\nTLBs are small (typically 16–512 entries) but highly effective due to temporal and spatial locality in most program behavior.",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Memory Management</span>"
    ]
  },
  {
    "objectID": "networks-intro.html",
    "href": "networks-intro.html",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#overview",
    "href": "networks-intro.html#overview",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "",
    "text": "Topic\nSlide Range\nNotes\n\n\n\n\n1. History & Fundamentals of the Internet\nSlides 1–16\nARPANET, TCP/IP, WWW, client-server, HTTP statelessness\n\n\n2. Circuit vs. Packet Switching\nSlides 17–23\nFDM, TDM, statistical multiplexing, delay tradeoffs\n\n\n3. Delays, Loss, Throughput\nSlides 24–35\nd_proc, d_queue, La/R, traceroute, throughput bottlenecks\n\n\n4. Protocol Layers and Encapsulation\nSlides 36–44\nLayer model, encapsulation, host/router/switch roles",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#historical-background-and-internet-foundations-slides-116",
    "href": "networks-intro.html#historical-background-and-internet-foundations-slides-116",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Historical Background and Internet Foundations (Slides 1–16)",
    "text": "Historical Background and Internet Foundations (Slides 1–16)\n\nEarly developments: ARPANET, Cyclades, and ALOHANet pioneered packet switching.\nThe Internet emerged as a global interconnection of autonomous systems using the TCP/IP protocol suite.\nTim Berners-Lee’s World Wide Web (1989–1991) introduced:\n\nA unified model of hyperlinked documents (HTML)\nThe HTTP protocol (stateless)\nURLs for addressing\nThe browser-server interaction model\n\nThe stateless nature of HTTP means each request is handled independently — servers do not retain memory of previous interactions.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#transmission-media-and-infrastructure-slides-1620",
    "href": "networks-intro.html#transmission-media-and-infrastructure-slides-1620",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Transmission Media and Infrastructure (Slides 16–20)",
    "text": "Transmission Media and Infrastructure (Slides 16–20)\n\nData transmission can occur over:\n\nCopper (UTP): electrical signals\nFiber optics: light pulses\nRadio: electromagnetic waves (Wi-Fi, LTE)\n\nFiber-optic links offer high bandwidth and low latency — widely used in backbone and undersea cables.\nSatellite communication has higher propagation delay (≥500 ms round-trip) due to distance (~36,000 km geostationary orbit).\nReal-world systems combine many media and technologies in layered infrastructure.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#circuit-switching-vs.-packet-switching-slides-1823",
    "href": "networks-intro.html#circuit-switching-vs.-packet-switching-slides-1823",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Circuit Switching vs. Packet Switching (Slides 18–23)",
    "text": "Circuit Switching vs. Packet Switching (Slides 18–23)\n\nCircuit switching: fixed, reserved paths (e.g. telephony)\n\nUses TDM (Time Division Multiplexing) or FDM (Frequency Division Multiplexing)\n\nPacket switching: data is broken into packets routed independently\n\nUses statistical multiplexing\nNo reservation of bandwidth; packets share the link dynamically\n\nTrade-offs of packet switching:\n\nMore efficient use of bandwidth under bursty traffic\nPotential for packet delay, loss, and reordering",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#network-performance-metrics-slides-2432",
    "href": "networks-intro.html#network-performance-metrics-slides-2432",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Network Performance Metrics (Slides 24–32)",
    "text": "Network Performance Metrics (Slides 24–32)\n\nFour types of delay:\n\nProcessing delay: time to examine packet header and perform checks\nQueueing delay: time waiting in the router buffer\nTransmission delay:\n\\[\nd_{\\text{trans}} = \\frac{L}{R}\n\\]\nwhere:\n\n\\(L\\): packet size (bits)\n\\(R\\): link bandwidth (bps)\n\nPropagation delay:\n\\[\nd_{\\text{prop}} = \\frac{d}{s}\n\\]\nwhere:\n\n\\(d\\): physical distance (meters)\n\\(s\\): signal propagation speed (m/s)\n\n\n\n\nTotal node delay:\n\\[\nd_{\\text{nodal}} = d_{\\text{proc}} + d_{\\text{queue}} + d_{\\text{trans}} + d_{\\text{prop}}\n\\]\n\n\nTraffic intensity and queue behavior:\nLet:\n\n\\(a\\): average packet arrival rate (packets/sec)\n\\(L\\): packet size (bits)\n\\(R\\): link bandwidth (bps)\n\nThen:\n\\[\n\\text{Traffic intensity} = \\frac{aL}{R}\n\\]\nInterpretation:\n\nIf \\(\\frac{aL}{R} \\geq 1\\): the queue grows without bound\nAs \\(\\frac{aL}{R} \\rightarrow 1\\): delay increases sharply\n\n\n\nEnd-to-end delay over multiple hops:\n\\[\nd_{\\text{end-to-end}} = \\sum_{i=1}^N (d_{\\text{proc},i} + d_{\\text{queue},i} + d_{\\text{trans},i} + d_{\\text{prop},i})\n\\]\nwhere \\(N\\) is the number of routers.\n\n\nTraceroute:\n\nUses IP TTL (Time-To-Live) field to probe each hop\nWhen TTL reaches zero, routers send an ICMP “Time Exceeded” message\nAllows measurement of round-trip time (RTT) per hop",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#throughput-slides-3335",
    "href": "networks-intro.html#throughput-slides-3335",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Throughput (Slides 33–35)",
    "text": "Throughput (Slides 33–35)\n\nThroughput: the rate at which data is successfully delivered (bps)\n\n\nTwo cases (Slide 34):\nIf:\n\n\\(R_S\\): server’s sending rate\n\\(R_C\\): client-side link rate\n\nThen:\n\nIf \\(R_S &lt; R_C\\), then \\(\\text{Throughput} = R_S\\)\nIf \\(R_S &gt; R_C\\), then \\(\\text{Throughput} = R_C\\)\n\n\\[\n\\text{Throughput} = \\min(R_S, R_C)\n\\]\n\n\nMulti-user sharing (Slide 35):\nIf 10 users share a backbone link of rate \\(R\\), and each has:\n\nSender link: \\(R_s\\)\nReceiver link: \\(R_c\\)\n\nThen per-connection throughput is:\n\\[\n\\text{Throughput} = \\min(R_s, R_c, \\frac{R}{10})\n\\]\n\nYes — definitely. The current version is clean and comprehensive, but if your goal is clarity + conciseness for study purposes, we can streamline it without sacrificing completeness.\nHere is a more succinct version of the same summary, optimized for use in study notes:",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#summary-protokollschichten-und-ihre-dienstmodelle-slides-36---45",
    "href": "networks-intro.html#summary-protokollschichten-und-ihre-dienstmodelle-slides-36---45",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Summary: Protokollschichten und ihre Dienstmodelle (Slides 36 - 45)",
    "text": "Summary: Protokollschichten und ihre Dienstmodelle (Slides 36 - 45)\n(Slides 36–44)\nThis final chapter introduces the layered architecture of the Internet. It explains how each protocol layer serves the one above and relies on the one below, and how encapsulation enables structured communication.\n\n\nLayering Motivation (Slide 36–37)\n\nNetworks are complex (hosts, routers, media, apps).\nSolution: Schichtenarchitektur for modular design.\nEach layer performs actions and uses only the services of the layer below.\n\n\n\n\nProtocol Layering: Foundations (Slides 36–38)\n\nDue to network complexity, functionality is divided into layers, each with clear responsibilities.\nA layer \\(k\\) uses only the services of layer \\(k-1\\):\n\\[\n\\text{Layer } k \\longrightarrow \\text{uses services of Layer } (k-1)\n\\]\nEach layer communicates vertically (service interface) and defines horizontal protocols (with its counterpart on the remote host).\nLayering enables:\n\nModularity\nReplaceability\nInteroperability\nAbstraction from hardware details\n\n\n\n\nThe Internet Stack (Slide 38–39)\n\n\n\n\n\n\n\n\nLayer\nFunction\nExamples\n\n\n\n\nApplication\nApplication protocols, user data\nHTTP, FTP, SMTP\n\n\nTransport\nProcess-to-process delivery\nTCP, UDP\n\n\nNetwork\nHost-to-host delivery, routing\nIP, ICMP\n\n\nData Link\nFrame-level delivery on local links\nEthernet, Wi-Fi, PPP\n\n\nPhysical\nTransmission of bits over the medium\nFiber, DSL, 5G\n\n\n\n\nLayers are identified by who communicates (e.g. processes, hosts, links).\nData is encapsulated step by step as it moves downward.\n\n\n\n\nProtocol Scope by Device (Slide 40)\n\n\n\nDevice\nImplements Up To\n\n\n\n\nHost\nAll 5 layers\n\n\nRouter\nNetwork layer (IP)\n\n\nSwitch\nData Link layer (MAC)\n\n\n\n\n\n\nEncapsulation (Slides 41–43)\nEach layer adds its own header (and possibly trailer). The result:\nFrame = [Data Link hdr] + [IP hdr] + [TCP hdr] + Message + [Trailer]\nAt the receiver, each layer removes its own header.\n\nOnly hosts process all layers.\nRouters read only IP headers.\nSwitches forward based on MAC addresses.\n\n\n\nOSI Model (Slide 44)\nA 7-layer reference model defined by ISO, used mostly for conceptual clarity.\n\n\n\nOSI Layer\nAdded vs. Internet Model\n\n\n\n\n7: Application\nMatches Internet’s application layer\n\n\n6: Presentation\nData format, compression, encryption\n\n\n5: Session\nDialog management\n\n\n4–1: Transport → Physical\nSame as in Internet stack\n\n\n\n\nInternet model simplifies OSI: layers 5–7 are often merged into the application.\n\n\nThis version keeps all major points but trims redundant explanation and tightens the wording for effective study reference.\nWould you like it exported to markdown or added to your ongoing Quarto notes?",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#unified-protocoll-stack-overview",
    "href": "networks-intro.html#unified-protocoll-stack-overview",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Unified Protocoll Stack Overview",
    "text": "Unified Protocoll Stack Overview\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nCommunication Endpoint\nData Unit Name\nWhat It Contains\nAdds Header/Footer?\nCan Split Data?\nTypical Protocols\n\n\n\n\nApplication\nApplications or processes (e.g., browser ↔︎ web server)\nMessage\nApp-level data (e.g. HTTP, SMTP)\nNo\nYes — application logic (e.g. file chunks)\nHTTP, FTP, SMTP, DNS, TLS, SSH, POP, IMAP\n\n\nTransport (TCP/UDP)\nSockets on end hosts (process ↔︎ process)\nSegment\nMessage + TCP/UDP header\nYes — transport header\nYes — TCP segments long messages\nTCP, UDP\n\n\nNetwork (IP)\nHosts or end systems (host ↔︎ host, abstracting from processes)\nPacket (or Datagram)\nSegment + IP header\nYes — network header\nYes — IP may fragment large packets\nIP (v4/v6), ICMP, IGMP\n\n\nData Link\nDirectly connected devices (e.g. Host ↔︎ Router)\nFrame\nPacket + MAC header + trailer (e.g. CRC)\nYes — frame header and trailer\nNo — one packet per frame\nEthernet, Wi-Fi (802.11), PPP, ARP\n\n\nPhysical\nPhysical interfaces (e.g., NICs, cables, radio) exchanging raw bits\nBits\nEncoded electrical/optical/radio signals\nN/A (not in software)\nNo — transmits one bit at a time\nDSL, Optical Fiber, Ethernet Cable, 5G",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "application-layer.html",
    "href": "application-layer.html",
    "title": "4  Intro to the Application Layer, Web and HTTP",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to the Application Layer, Web and HTTP</span>"
    ]
  },
  {
    "objectID": "application-layer.html#overview",
    "href": "application-layer.html#overview",
    "title": "4  Intro to the Application Layer, Web and HTTP",
    "section": "",
    "text": "Slide\nTitle / Chapter Heading\nContent Covered in This Chapter\n\n\n\n\n1\nTitle Slide\nVorlesung N02 – Artur Andrzejak\n\n\n2\nProtokollschichten und ihre Dienstmodelle (Review)\nMotivation for layering, protocol stack overview, encapsulation, OSI model\n\n\n11\nDas Web und HTTP\nWeb as an application, HTTP request/response, URLs, message formats\n\n\n20\nHTTP – Fortgeschrittene Konzepte\nPersistent connections, cookies, status codes, conditional GET\n\n\n30\nGrundlagen von Netzwerkanwendungen\nApplication architectures (Client-Server, P2P), sockets, addressing\n\n\n37\nZwei Grundlegende Internet-Protokolle\nTCP and UDP compared: properties, use cases\n\n\n38\n(End of main material)\nTable summarizing example applications and their underlying protocols\n\n\n\n\n\nVisual Flow:\n\nSlides 2–10: Foundations of the Internet stack (5-layer model), encapsulation, and implementation across hosts/routers.\nSlides 11–19: Introduction to the Web and HTTP — how web clients and servers communicate, HTTP syntax.\nSlides 20–29: Deeper into HTTP — efficiency issues, cookies, status handling, caching mechanisms.\nSlides 30–36: What constitutes a network application — roles of processes, clients/servers, socket interfaces.\nSlides 37–38: TCP vs UDP — core transport-layer protocols and which applications use them.\n\n\nProgression:\n\nIt begins with protocol layering as a general framework,\nMoves to a real-world application: the Web,\nAdds depth by covering advanced HTTP behavior,\nThen generalizes back to all network apps: architecture, sockets, addressing,\nAnd ends with transport-level choices (TCP vs UDP).\n\nAbsolutely. Below is a more comprehensive and didactically structured summary of vlN02-ibn.pdf, suitable for integration into your note-taking system. It covers the main thread (slides 11–29) and integrates relevant clarifying detours for long-term retention.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to the Application Layer, Web and HTTP</span>"
    ]
  },
  {
    "objectID": "application-layer.html#ibn-vorlesung-02-http-und-die-anwendungsschicht",
    "href": "application-layer.html#ibn-vorlesung-02-http-und-die-anwendungsschicht",
    "title": "4  Intro to the Application Layer, Web and HTTP",
    "section": "IBN – Vorlesung 02: HTTP und die Anwendungsschicht",
    "text": "IBN – Vorlesung 02: HTTP und die Anwendungsschicht\nSource: vlN02-ibn.pdf (slides 11–29) Focus: Practical introduction to the Application Layer of the Internet stack using HTTP and the Web as central examples.\n\n\nContext and Approach\nYou started at slide 11, skipping the review of protocol layering (slides 2–10), which was already covered in vlN01-ibn.pdf. The lecture introduces how HTTP functions as a real-world example of an application-layer protocol built on top of TCP.\n\n\n\nSlide 11 – Das Web und HTTP\n\nThe Web is a distributed application using the HTTP protocol, layered over TCP.\nA single web page typically includes:\n\nA main HTML file\nAdditional embedded resources (images, CSS, JS, etc.)\n\n\n\n\n\nSlide 12 – Übersicht und Begriffe\n\nDefines key terms:\n\nBrowser: the HTTP client\nWeb server: provides content via HTTP\nURL: encodes hostname, protocol, and path to resource\n\nHTTP clients usually connect to servers on port 80.\n\n\n\n\nSlide 14 – Hypertext Transfer Protocol (HTTP)\n\nHTTP runs over TCP and is a stateless protocol.\nIn its basic form (HTTP/1.0), each request requires a new TCP connection.\nLater versions (HTTP/1.1+) support persistent connections.\n\n\n\n\nSlide 15 – HTTP-Request (Beispiel)\n\nStructure of a simple GET request:\nGET /index.html HTTP/1.1\nHost: www.example.com\nUser-Agent: ...\nGET requests contain no body.\n\n\n\n\nSlide 16 – Allgemeines Format der Request-Nachricht\n\nEvery HTTP request consists of:\n\nRequest line (method, path, HTTP version)\nHeader fields (e.g. Host, User-Agent, Content-Length)\nOptional body (only for POST, PUT, etc.)\n\n\n\n\n\nSlide 17 – HTTP-Request-Methoden\n\nSupported methods:\n\nGET: parameters in URL\nPOST: parameters in body\nHEAD: like GET but without response body\nPUT, DELETE: less common, used in RESTful APIs\n\nHTTP is semantically extensible, but the method determines message structure.\n\n\n\n\nSlide 18 – HTTP-Response (Beispiel)\n\nResponse structure includes:\n\nStatus line (e.g., HTTP/1.1 200 OK)\nHeader fields (e.g., Content-Type, Content-Length)\nOptional body (HTML, image, etc.)\n\n\n\n\n\nSlide 19 – Allgemeines Format der Response-Nachricht\n\nResponse message layout:\n\nStatus line\nHeaders\nBlank line\nOptional entity body\n\n\n\n\n\nSlide 21 – HTTP mit Telnet simulieren\n\nDemonstrates that HTTP is plain-text based.\nYou can use telnet to manually type HTTP requests.\nThis reinforces the protocol’s human-readable structure.\n\n\n\n\nSlide 22 – Statuscodes und Statusnachrichten\n\nHTTP status line contains:\n\nProtocol version\nNumeric status code (e.g., 200, 404)\nOptional text message (e.g., “OK”, “Not Found”)\n\nClients should rely on the numeric code, not the message text.\n\n\n\n\nSlide 23 – HTTP Verbindungstypen\n\nDistinguishes between:\n\nNichtpersistente Verbindungen: new TCP connection per object (inefficient)\nPersistente Verbindungen: single TCP connection reused for multiple objects (default in HTTP/1.1)\n\n\n\n\n\nSlide 24 – Antwortzeit bei nichtpersistenten Verbindungen\n\nNon-persistent setup incurs at least:\n\n1 RTT for TCP handshake\n1 RTT for HTTP request-response\n\nThis adds up to 2 RTTs + transfer time per object.\nFor pages with many small resources, the overhead becomes significant.\n\n\n\n\nSlide 25 – Vergleich: Persistente vs. Nichtpersistente Verbindungen\n\nPersistent connections reduce:\n\nLatency\nResource usage\nNetwork congestion\n\nNon-persistent connections introduce repeated setup and teardown.\n\n\n\n\nSlide 26 – Benutzerzustand via Cookies\n\nHTTP is stateless, but cookies allow servers to maintain state.\nMechanism:\n\nServer sets a cookie via Set-Cookie header\nBrowser stores it and includes it in future requests using the Cookie header\n\nEnables sessions, user tracking, and authentication\n\n\n\nRelated Discussion: Cookies and Logins\n\nLogin systems rely on cookies to associate a session ID with a user.\nThe cookie itself does not contain sensitive data, only an identifier.\nThe session data lives on the server.\n\n\n\n\n\nSlide 27 – Beispiel: HTTP und Cookies\n\nFirst visit: server sets a cookie (Set-Cookie: ID=12345)\nSecond visit: client automatically sends (Cookie: ID=12345)\nServer identifies the session using this ID.\n\n\n\n\nSlide 28 – HTTP: Bedingtes GET\n\nIntroduces the If-Modified-Since header:\n\nClient asks: “Has this file changed since &lt;timestamp&gt;?”\nIf no change, server responds with 304 Not Modified and no body.\nSaves bandwidth and improves performance.\n\nBasis for browser caching and proxy validation.\n\n\n\n\nSlide 29 – Internet-Protokollstapel\n\nOverview of the 5-layer Internet protocol stack:\n\n\n\n\n\n\n\n\n\nLayer\nExample Protocols\nFunction\n\n\n\n\nApplication Layer\nHTTP, FTP, SMTP\nNetwork applications\n\n\nTransport Layer\nTCP, UDP\nReliable or best-effort process communication\n\n\nNetwork Layer\nIP, Routing Protocols\nPacket forwarding across networks\n\n\nData Link Layer\nEthernet, PPP\nLocal frame delivery between adjacent nodes\n\n\nPhysical Layer\n(depends on medium)\nBit transmission over hardware links\n\n\n\n\nEmphasizes modularity and encapsulation.\n\n\n\nSlide 30 – Abschnittsübergang: Grundlagen von Netzwerkanwendungen\nTitle slide:\n\nGrundlagen von Netzwerkanwendungen (Architekturen, Sockets, Protokolle)\n\n\nFunction:\n\nMarks the transition from a concrete case study (HTTP) to general concepts in the application layer.\nSets the stage for understanding how any application-level protocol is built, not just HTTP.\n\n\n\n\n\nSlide 31 – Netzwerkanwendungen sind …\n\nCore Insight:\n\nNetwork applications consist of processes (not just hosts) that exchange messages.\nThese processes run at the end systems only — routers and switches do not participate in the application logic.\n\n\n\nArchitecture View:\n\nApplication logic exists only at the network edges (hosts), reinforcing the end-to-end principle.\nRouters forward packets; they do not run web servers or browsers.\n\n\n\n\n\nSlide 32 – Prozesskommunikation\n\nKey Idea:\n\n“It’s not the computers, but the processes that communicate.”\n\n\nOn the same host → Interprocess Communication (IPC)\nAcross hosts → Message exchange over TCP or UDP\nProcesses use sockets to communicate via the transport layer.\n\n\n\nTerminology:\n\nApplication process communicates over a socket.\nSocket is the OS-provided API to send/receive messages across the network.\n\n\n\n\n\nSlide 33 – Architektur: Client-Server vs. Peer-to-Peer (P2P)\n\nClient-Server:\n\nCentral server, always on, often with a fixed IP.\nClients initiate communication.\nExamples: HTTP, SMTP, FTP\n\n\n\nPeer-to-Peer:\n\nNo central server; every peer can be both client and server.\nMore scalable, but more complex (NAT traversal, coordination).\nExamples: BitTorrent, VoIP (classic Skype)\n\n\n\n\n\nSlide 34 – Prozesse: Client und Server\n\nRoles defined by behavior:\n\nClient: initiates communication\nServer: waits to be contacted\n\n\n\nImportant:\n\nThese are roles, not hardware definitions.\nIn P2P, a node can act as client in one interaction, server in another.\n\n\n\n\n\nSlide 35 – Sockets: Schnittstellen\n\nSockets:\n\nThe programming interface between the application process and the transport layer.\nA socket is like a door through which the process sends/receives data.\n\n\n\nOS abstraction:\n\nOS handles TCP/UDP details.\nApplication uses send(), recv(), or read()/write() on the socket.\n\n\n\n\n\nSlide 36 – Addressieren von Prozessen\n\nProblem:\n\nIP address identifies a host, not a specific process.\n\n\n\nSolution:\n\nUse (IP address, Port number) to identify a process.\nKnown ports (e.g. 80 for HTTP, 443 for HTTPS) identify standard services.\n\n\n\n\n\nSlide 37 – Zwei grundlegende Internet-Protokolle: TCP und UDP\n\n\n\nFeature\nTCP\nUDP\n\n\n\n\nConnection setup\nYes (3-way handshake)\nNo\n\n\nReliability\nYes\nNo\n\n\nOrdering\nGuaranteed\nNo\n\n\nCongestion control\nYes\nNo\n\n\nUse case examples\nHTTP, FTP, email\nDNS, VoIP, streaming\n\n\n\n\nTCP is used when reliability and ordering matter.\nUDP is used when speed and simplicity are more important than reliability.\n\n\n\n\nSlide 38 – Beispiele für Anwendungen\n\nProtocol usage by application:\n\n\n\nApplication\nProtocol\n\n\n\n\nHTTP, FTP, SMTP\nTCP\n\n\nDNS (queries)\nUDP\n\n\nDNS (zone transfer)\nTCP\n\n\nVoIP, video stream\nUDP\n\n\n\n\nSome apps use both, depending on use case (e.g. DNS).\n\n\n\n\n\nClarifying Detours and Triggers\n\n1. “Do HTTP servers need to know the client’s IP address?”\nTriggered by: slide 35 (Sockets) Clarification:\n\nNo, not usually. The OS handles delivery.\nThe server can access the client IP via accept() if needed, but it’s optional.\n\n\n\n2. “Is it realistic to build your own HTTP server?”\nTriggered by: understanding sockets in slide 35 Discussion:\n\nYes — very realistic.\nA minimal HTTP server can be written in under 50 lines of Python using just the socket API.\nFocus is on parsing requests, forming responses, serving files, and handling concurrency.\n\n\n\n3. “Why do Apache and Nginx exist if HTTP is simple?”\nTriggered by: realization that building a server is feasible Clarification:\n\nIndustrial servers are engineered for performance, security, flexibility, and scaling.\nThey handle advanced features like TLS, compression, load balancing, and dynamic routing.\n\n\n\n4. “Can an HTTP server listen on multiple ports?”\nTriggered by: slide 36 (port numbers) Clarification:\n\nYes — it’s technically trivial (e.g. bind() on multiple sockets).\nOften done for development (8080), reverse proxies, or alternate services.\n\n\n\n5. “Do P2P applications prefer UDP over TCP?”\nTriggered by: slide 33 (P2P architectures) Clarification:\n\nMany do prefer UDP due to NAT traversal, lower latency, and flexible retransmission control.\nBut some, like BitTorrent, still use TCP for reliability.\n\n\n\n\nPractical Experiment\nYou tested this on your Quarto preview server (localhost:3475) and used telnet to manually send a GET / request. The server responded with:\nHTTP/1.1 200 OK\nContent-Type: text/html\nContent-Length: ...\nThis confirmed:\n\nHTTP is text-based\nThe Quarto server is a fully functional HTTP server\nThe response body was a real HTML document rendered by the browser\n\n\n\n\n1. Chunking and Large Files\nWhat initiated it: Slide 18 showed a large HTML response. You asked:\n\n“If the file is large, is it split into multiple HTTP messages?”\n\nWhat we explored:\n\nHTTP responses are not split at the application level\nTCP handles segmentation\nChunked transfer encoding exists for streaming, but still within one HTTP response\n\n\n\n\n2. Opening Local HTML Files in Browser\nWhat initiated it: You asked how browsers handle local .html files opened by double-clicking.\nWhat we explored:\n\nIf all resources (CSS, JS) are embedded, the file renders correctly\nIf external files are fetched via HTTP, they fail unless a local server is running\nBrowsers block fetch() from file:// for security\n\n\n\n\n3. What is an SPA (Single Page Application)?\nWhat initiated it: You observed that SPAs avoid reloading full HTML pages and instead dynamically update content.\nWhat we explored:\n\nSPAs load one HTML file and update the DOM using JavaScript\nNo page reloads = smoother experience\nCommonly powered by frameworks like React, Vue, etc.\n\n\n\n\n4. JSON, REST, and GraphQL\nWhat initiated it: You asked how SPAs fetch data and render views without loading new HTML pages.\nWhat we explored:\n\nSPAs fetch JSON from the server\nREST APIs expose fixed endpoints (/api/posts)\nGraphQL allows flexible, structured queries ({ post { title } })\nBackend sends data, not HTML\n\n\n\n\n5. Role of PostgreSQL and Backend\nWhat initiated it: You asked how GraphQL or REST interfaces connect to a PostgreSQL database.\nWhat we explored:\n\nThe backend server receives GraphQL or REST requests\nIt queries PostgreSQL for data\nResponds with JSON to the frontend\n\n\n\n\n6. Authentication and Password Handling\nWhat initiated it: You asked how user login works — and whether passwords are visible to the server.\nWhat we explored:\n\nPasswords are hashed using bcrypt and stored in the database\nOnly hashed values are stored; plain passwords are never saved\nAfter login, the server issues:\n\nA session cookie, or\nA JWT (JSON Web Token)\n\nThe frontend uses the token to authenticate future requests\n\n\n\n\n7. HTTPS and Security\nWhat initiated it: You noticed the password is sent in plain text within the HTTP body.\nWhat we explored:\n\nWithout HTTPS, this is dangerous — passwords can be intercepted\nHTTPS encrypts the entire transmission (headers + body)\nModern login flows require HTTPS for secure password handling\n\n\n\n\n8. Final Integration: Full-Stack SPA Project\nWhat initiated it: You asked whether this could be integrated into a learning project.\nWhat we designed:\n\nA minimal blog platform (MiniPost) with:\n\nSPA frontend (React or Svelte)\nGraphQL backend (Node.js)\nPostgreSQL database\nLogin/authentication (JWT + bcrypt)\n\nDesigned as a hands-on way to tie everything together\n\n\n\n\n1. What is a TCP segment?\n\nA TCP segment = TCP header + data payload\nSent over IP; reassembled on the receiving side using SEQ/ACK numbers\n\n\n\n2. What is a byte stream?\n\nTCP provides a reliable, ordered byte stream\nNo inherent message boundaries — just a flow of bytes\nContrast with message-oriented protocols like UDP\n\n\n\n3. How does TCP know transmission is over?\n\nSender signals with a FIN segment\nConnection is closed using a 4-step FIN/ACK handshake\n\n\n\n4. What does “contiguous bytes received” mean?\n\nTCP only acknowledges data that has been received in order, without gaps\nIf segments arrive out of order, ACK does not advance until missing parts are filled in\n\n\n\n5. Duplicate segment handling\n\nIf the same segment arrives twice (e.g. delayed + retransmitted), TCP detects overlap using SEQ numbers and silently discards duplicates\n\n\n\n6. Do segments usually arrive in order?\n\nYes, most of the time — but TCP is built to tolerate:\n\nModerate reordering\nDelayed or duplicate segments\n\n\n\n\n7. When and how retransmission occurs\n\nFast Retransmit is triggered by 3 duplicate ACKs\nTimeout-based retransmission (RTO) occurs when ACKs don’t arrive\nFast retransmit is called “fast” because it reacts quicker than waiting for RTO\n\nCertainly — here is a clean, structured summary of what we covered in this thread from Slide 39 onward. I’ve grouped related slides into coherent thematic units and removed redundant repetitions, while retaining all core insights.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to the Application Layer, Web and HTTP</span>"
    ]
  },
  {
    "objectID": "ip-layer-subnetting.html",
    "href": "ip-layer-subnetting.html",
    "title": "5  IP Layer & Subnetting",
    "section": "",
    "text": "Overview\nThis lecture begins the deep dive into the Network Layer of the Internet stack — specifically the Internet Protocol (IP). We analyze how data is transmitted between hosts across networks, how datagrams are structured, and how IP addresses define logical and physical boundaries within networks.\nSeveral side explorations were included to clarify underlying technologies (e.g. ARP, DHCP, NAT) and real-world applications (e.g. dormitory network issues, personal Wi-Fi routing).",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IP Layer & Subnetting</span>"
    ]
  },
  {
    "objectID": "ip-layer-subnetting.html#summary-of-vln03-internet-protocol-and-addressing-slides-317",
    "href": "ip-layer-subnetting.html#summary-of-vln03-internet-protocol-and-addressing-slides-317",
    "title": "5  IP Layer & Subnetting",
    "section": "Summary of VLN03 – Internet Protocol and Addressing (Slides 3–17)",
    "text": "Summary of VLN03 – Internet Protocol and Addressing (Slides 3–17)\n\n1. HTTP Traffic and Wireshark (Slides 3–7)\nWireshark is used to capture and inspect real HTTP traffic.\n\nKey points:\n\nHTTP messages are captured and decoded by Wireshark.\nThese are transported over TCP, which is encapsulated in IP, which is wrapped in Ethernet frames.\n\n\n\nReassembly in Wireshark:\nWireshark reconstructs complete HTTP messages from multiple TCP segments using internal state tracking.\n\n\n\nKey term: PDU (Protocol Data Unit)\nA PDU is the unit of data defined at a specific protocol layer.\n\n\n\n\n\n\n\n\nLayer\nPDU Name\nExample\n\n\n\n\nApplication Layer\nMessage\nHTTP request or response\n\n\nTransport Layer\nSegment / Datagram\nTCP segment, UDP datagram\n\n\nNetwork Layer\nDatagram\nIP packet\n\n\nLink Layer\nFrame\nEthernet frame\n\n\nPhysical Layer\nBits\nElectrical or optical signals\n\n\n\n\n\n\n\n2. Introduction to the IP Layer (Slides 8–10)\nWe zoom into the Network Layer of the Internet stack.\n\nMain components of the network layer:\n\nIP protocol:\n\nPacket format, addressing, and forwarding\nStateless, best-effort delivery\n\nRouting protocols:\n\nDetermine forwarding tables (e.g. RIP, OSPF, BGP)\n\nICMP:\n\nUsed for diagnostics (e.g. ping, traceroute)\n\n\n\n\n\nKey term: Datagram\nA datagram is a self-contained, independently routed network-layer packet. In IP networks, it refers to an IP packet that includes both header and payload.\n\nConnectionless\nMay be fragmented\nCarries no session state\n\n\nIP packets = IP datagrams (used interchangeably)\n\n\n\n\n\n3. Structure of an IP Datagram (Slide 11)\nWe study the fields of the IPv4 header.\n\nIP Header Fields (simplified):\n\n\n\n\n\n\n\n\nField\nSize (bits)\nPurpose\n\n\n\n\nVersion\n4\nIP version (usually 4)\n\n\nHeader Length\n4\nIn 32-bit words\n\n\nDS/ECN\n8\nDifferentiated Services / Congestion\n\n\nTotal Length\n16\nHeader + payload length\n\n\nIdentification\n16\nFragmentation ID\n\n\nFlags\n3\nFragment control\n\n\nFragment Offset\n13\nPosition of fragment\n\n\nTTL\n8\nTime to live\n\n\nProtocol\n8\nHigher-layer protocol (e.g. TCP = 6)\n\n\nHeader Checksum\n16\nDetects corruption in header\n\n\nSource IP\n32\nIP of sender\n\n\nDestination IP\n32\nIP of receiver\n\n\nOptions (optional)\nvariable\nRarely used\n\n\n\n\n\n\n\n4. IP Fragmentation (Slides 12–13)\n\nWhy fragmentation exists:\n\nEthernet and other link-layer technologies impose an MTU (Maximum Transmission Unit).\nIP allows a large datagram to be split into smaller fragments if the MTU is too small.\n\n\n\nFragmentation fields:\n\nIdentification: Shared across all fragments of a datagram\nFragment Offset: Position (in 8-byte units)\nMF (More Fragments): Flag set to 1 for all fragments except the last\n\n\nOnly the destination host reassembles fragments. Routers never do.\n\n\n\nExample (simplified):\nA 3072-byte datagram sent over a 1200-byte MTU link:\n\n\n\nFragment\nOffset\nMF\nData bytes\n\n\n\n\n1\n0\n1\n960\n\n\n2\n120\n1\n960\n\n\n3\n240\n0\n1020\n\n\n\n\n\n\n\n5. Interfaces and Addressing (Slides 14–15)\n\nKey concepts:\n\nAn IP address is assigned to a network interface, not the host as a whole.\nA router has multiple interfaces, each on a different subnet.\nA host usually has one IP.\nDevices in the same subnet share the same address prefix.\n\n\n\nReference network diagram from slide 15:\n\nRouter interfaces:\n\n223.1.1.4 (Subnet A, left)\n223.1.2.9 (Subnet B, right)\n223.1.3.27 (Subnet C, bottom)\n\nSubnets and hosts:\n\n\n\n\nSubnet\nHosts\nRouter IP\n\n\n\n\nA\n223.1.1.1 – 1.3\n223.1.1.4\n\n\nB\n223.1.2.1 – 2.2\n223.1.2.9\n\n\nC\n223.1.3.1 – 3.2\n223.1.3.27\n\n\n\nAll IPs within a subnet share the same /24 prefix.\n\n\n\n\n6. LANs, WLANs, and Subnets (Discussion and Detours)\nTriggered by slide 15 and related questions.\n\nClarifications:\n\nA LAN is a link-layer broadcast domain (Ethernet, WLAN).\nA subnet is a logical grouping of IPs — typically matches a LAN, but not always.\nA WLAN is a type of LAN using wireless physical media.\n\n\n\n\nKey term: ARP (Address Resolution Protocol)\nARP resolves IP addresses to MAC addresses on a LAN.\n\nA device sends a broadcast:\nWho has 192.168.1.5?\nThe target replies with its MAC address.\nThe result is cached in the ARP table.\n\n\nWithout ARP, IP-based communication over Ethernet is impossible.\n\n\n\n\nARP Spoofing\nA malicious device sends fake ARP replies:\n\n“I am 192.168.1.1 — here’s my MAC”\nCan hijack traffic (man-in-the-middle)\n\nThis only works on shared LANs without port isolation.\n\n\n\n\n7. Dorm Network Architecture (Applied Detour)\nTriggered by IP addressing and subnet questions\n\nDorm connections are likely per-port VLANs\nEthernet switch: exposes multiple MACs/IPs to Nexabit\nWi-Fi AP (with NAT): hides multiple devices behind one IP\nNexabit bans switches/APs due to fragility, not inherent danger\n\n\nYour dorm disconnection incident may have occurred due to multiple devices behind a dumb switch overwhelming fragile infrastructure — not misuse on your part.\n\n\n\n\n8. Subnet Hardware Definition (Slide 17)\nA subnet consists of all interfaces connected to the same physical link-layer medium, excluding routers.\nVisual trick: if you “cut away” each router’s interfaces, the remaining connected islands are subnetworks.\nIn the slide:\n\nRegions 1–3: LAN subnets (hosts + router)\nRegions 4–6: router-router links (point-to-point subnets)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IP Layer & Subnetting</span>"
    ]
  },
  {
    "objectID": "ip-layer-subnetting.html#summary-of-vln03-slides-1823",
    "href": "ip-layer-subnetting.html#summary-of-vln03-slides-1823",
    "title": "5  IP Layer & Subnetting",
    "section": "Summary of VLN03 — Slides 18–23",
    "text": "Summary of VLN03 — Slides 18–23\n(IP Address Blocks, Subnets, and Classful Addressing)\n\nSubnets and Minimal Address Blocks\nWe revisit the subnet diagram from slide 15 and ask: What is the minimal CIDR block that covers all required IPs for Subnet A and B?\nAlthough it might seem sufficient to use the numerical range of required IPs (e.g. .1–.4), IP subnets must follow strict rules:\n\nA subnet must be a CIDR-aligned block: its size must be a power of two, and its starting address must be divisible by that size.\nTherefore, to include 223.1.1.4, Subnet A requires a /29 block: 223.1.1.0 – 223.1.1.7\nTo include 223.1.2.9, Subnet B requires a /28 block: 223.1.2.0 – 223.1.2.15\n\nThese blocks ensure binary-aligned network prefixes suitable for routing.\n\n\nNetID and HostID Structure\nAn IP address is logically split into:\n\nNetID — determines the network or subnet\nHostID — identifies a host within that network\n\nThe boundary is defined by the CIDR prefix (e.g. /24), where the first n bits are the NetID, and the remaining 32−n bits are the HostID.\nThis structure enables:\n\nScalable routing — routers forward based only on the NetID\nLocal autonomy — host addresses are managed internally\n\nRouting efficiency increases because routers don’t need entries for individual hosts — only for the networks.\n\n\nAddress Allocation and Prefix Sizing\nHow long should the NetID be? It depends on the use case:\n\nA long NetID (e.g. /27) allows many networks with few hosts\nA short NetID (e.g. /16) allows fewer networks but many hosts per network\n\nThis trade-off determines how many subnets or hosts fit in a given address block. Organizations receiving a larger block (e.g. /16) can internally create finer subdivisions using longer prefixes (/24, /28, etc.).\n\n\nClassful Addressing\nHistorically, IP addresses were divided into rigid classes, which defined fixed NetID/HostID splits:\n\n\n\n\n\n\n\n\n\n\nClass\nNetID Bits\nHostID Bits\nAddress Range\nNotes\n\n\n\n\nA\n8\n24\n1.0.0.0 – 126.255.255.255\nLarge orgs\n\n\nB\n16\n16\n128.0.0.0 – 191.255.255.255\nMid-size\n\n\nC\n24\n8\n192.0.0.0 – 223.255.255.255\nSmall orgs\n\n\nD\n—\n—\n224.0.0.0 – 239.255.255.255\nMulticast\n\n\n\nThis system was easy for routers to interpret (based on the first few bits), but led to massive address waste. For example, a company needing 500 IPs couldn’t use a /24 (too small), so it had to request a /16 block — wasting over 60,000 addresses.\n\n\n\nDetours and Clarifications\n\nWhat is a CIDR-aligned block?\nA block of size \\(2^k\\) must begin at an address divisible by \\(2^k\\) and span exactly that many addresses. Arbitrary ranges like .1–.4 are not valid subnet definitions.\n\n\nWhat is x.x.x.x/y notation?\nCIDR notation explicitly states the NetID length. /24 means 24 bits of network, 8 bits of host. The prefix length determines both address block size and routing behavior.\n\n\nWhy longest prefix match?\nWhen multiple routing entries match a destination, routers choose the one with the longest (most specific) prefix. This ensures fine-grained routing within larger aggregate blocks.\n\n\nHow does a packet reach the final host?\nWhen a packet reaches the destination subnet, the router uses ARP to resolve the destination IP to a MAC address, then sends the packet as an Ethernet frame. The target host accepts the frame based on MAC address filtering.\n\n\nWhat happens on a shared medium without a switch?\nOn a shared Ethernet or Wi-Fi network, all hosts receive all frames. Only the device whose NIC matches the destination MAC address accepts the packet; the rest discard it silently.\n\n\nDoes each NetID correspond to a single subnet?\nNot necessarily. A large NetID (e.g. /16) can be internally subnetted into multiple smaller blocks (/24, /27, etc.). Subnetting is hierarchical and flexible under CIDR.\n\n\nCould organizations subnet class-based address blocks?\nYes. Even under the old class-based system, organizations could borrow bits from the HostID portion to create subnets internally. For example, a Class B /16 block could be subnetted into 256 /24 blocks using a subnet mask like 255.255.255.0. This allowed structured internal networks despite the rigidity of the external class system.\nCertainly — here is the same answer, reformatted according to your project’s preferences (no boldface in headings, clean structure, sparing emphasis in body text).",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IP Layer & Subnetting</span>"
    ]
  },
  {
    "objectID": "ip-layer-subnetting.html#summary-of-vln03-slides-2438",
    "href": "ip-layer-subnetting.html#summary-of-vln03-slides-2438",
    "title": "5  IP Layer & Subnetting",
    "section": "Summary of VLN03 — Slides 24–38",
    "text": "Summary of VLN03 — Slides 24–38\n(From classless addressing to routing and network segmentation at Uni Heidelberg)\n\n\n1. From Classful to Classless Addressing (Slides 24–25)\n\nProblems with classful addressing\n\nWaste of addresses: e.g. a Class B allocation (65k addresses) for an org needing only a few hundred\nNo aggregation: every network required a separate route — routing tables became large and unscalable\nChaotic distribution: no topological coherence; blocks were allocated sequentially\nFrequent updates: every new network affected global routing tables\n\n\n\nSolution: CIDR (Classless Inter-Domain Routing)\nCIDR was introduced in 1993 to allow arbitrary prefix lengths:\na.b.c.d/x   →  x = number of NetID bits\nCIDR enables:\n\nFine-grained allocation (e.g. /23, /26, /30)\nRoute aggregation in global routing tables:\n\ne.g. 192.168.0.0/22 covers 4 contiguous /24 networks\n\nScalable design: organizations get only what they need; global tables remain compact\n\n\n\n\n\n2. Routing hierarchy and subnetting (Slides 26–28)\n\nHierarchical routing\n\nGlobal routers (ISPs) see only top-level blocks (e.g. /16)\nInstitutional routers handle internal subnetting and distribution\nLocal routers (e.g. in buildings) know the details of subnet-to-interface mapping\n\n\n\nSubnetting strategy\nFrom a CIDR block (e.g. /16), an institution can create multiple smaller subnets (/24, /22, etc.) Bits from the HostID are borrowed to form a SubnetID.\n\n\nAddress structure with subnetting\n\n\n\nField\nBit Count\nDescription\n\n\n\n\nNetID\ne.g. 16\nAssigned by provider (e.g. 129.206)\n\n\nSubnetID\ne.g. 8\nInternal routing (e.g. Informatik = 78)\n\n\nHostID\ne.g. 8\nHost within the subnet (e.g. .42)\n\n\n\nExample: 129.206.78.42 NetID = 129.206, SubnetID = 78, HostID = 42\n\n\n\n\n3. Subnet masks and CIDR application (Slides 29–31)\n\nWhat is a subnet mask?\nA 32-bit value used to separate:\n\nNetwork + Subnet part (leftmost bits)\nHost part (remaining bits)\n\n\n\nNotation\n\nCIDR: /x, where x = number of 1s in mask\nDotted decimal: 255.255.255.0 ≡ /24\n\n\n\nExample calculation\nIP address:      134.155.48.10\nSubnet mask:     255.255.255.0 (/24)\n→ Subnet base:   134.155.48.0\n\n\nUse case\nHosts check if another IP is in the same subnet using a bitwise AND with the subnet mask. If not → packet forwarded to default gateway.\n\n\n\n\n4. Visibility and internal routing (Slides 32–33)\n\nSubnetID is local\nOnly the top-level prefix (e.g. 129.206.0.0/16) is visible globally. Internal subnet structure (e.g. .78.0/24, .61.0/24) is managed inside the university.\n\n\nExample from Uni Heidelberg\n\nPublic /16 blocks:\n\n129.206.0.0/16 (north of Neckar)\n147.142.0.0/16 (south of Neckar incl. Mathematikon)\n\nSubnet allocations:\n\n\n\n\nDepartment\nSubnet\nSize\n\n\n\n\nMedizin\n/18\n16,384 IPs\n\n\nPhysik\n/18\n16,384 IPs\n\n\nInformatik\n/22\n1,024 IPs\n\n\n\n\nOne AG (research group) was assigned:\n129.206.61.39 – 129.206.61.63\n→ 24 usable public IPs, not a full subnet, but a carved-out range from the /24\n\n\n\n\n\n5. Physical topology of the Mathematikon (Slides 34–35)\n\nTwo routers in the basement act as:\n\nGateways to the university backbone\nEntry points to the building network\n\nIPs of routers: .1 in each subnet (e.g. 129.206.78.1)\n13 CISCO Catalyst 4500 switches across floors\nConnected via fiber → act as one logical switching fabric\nEnables VLANs to span all floors\n\n\n\n\n6. VLANs and subnet spanning (Slide 36)\n\nVLAN (Virtual LAN)\nA VLAN is a logically segmented subnet that can span multiple physical switches.\n\nSwitch ports are assigned to VLANs (e.g. VLAN 78 → 129.206.78.0/24)\nProvides:\n\nTraffic isolation\nSecurity\nFlexibility: any office on any floor can be part of the same subnet\n\n\nVLANs allow centralized IP and subnet control across a distributed physical environment.\n\n\n\n\n7. End of subnetting discussion (Slide 37)\nThis slide links to a video: Cisco Router Training 101, starting at 16:30 — reviewing subnet masks and routing behavior.\n\n\n\n8. Transition to routing (Slide 38)\nThe final slide begins the topic of packet forwarding and routing tables, continued in VLN04.\n\n\n\nDetours and enrichments\n\nHow a packet reaches a subnet host\nPrompted by: Slide 28 — internal subnet structure\nWe analyzed:\n\nGlobal routing to /16 block\nInternal forwarding to subnet router\nFinal delivery via ARP and Ethernet to the destination host\n\n\n\n\nDo routers use MAC addresses?\nPrompted by: Routing behavior\nClarified:\n\nRouters have MAC addresses on each interface\nMAC addresses are rewritten per hop (layer 2), while IP stays constant (layer 3)\n\n\n\n\nReverse proxies and ngrok\nPrompted by: Slide 33 — public IPs in Informatik\nWe discussed:\n\nReverse proxies (nginx, Apache, etc.) forward requests from clients to internal servers\nngrok acts like a hosted reverse HTTP proxy using a tunnel to expose local services\nDifferences between public IP access and NAT + proxy setups\n\n\n\n\nWhy research groups might get public IPs\nPrompted by: Slide 33 — “meiner AG wurden 24 Adressen zugeteilt”\nWe explored:\n\nUniversities with large legacy blocks can afford to assign public IPs directly\nBenefits: easy incoming connections, public DNS mapping, no NAT hassle\nRisk: requires strong firewall policies\n\n\n\n\nWhat is ARP?\nPrompted by: Final packet delivery on LAN\nDefined ARP as the Address Resolution Protocol — maps IP to MAC within a subnet using broadcast, enabling packet delivery at layer 2.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IP Layer & Subnetting</span>"
    ]
  },
  {
    "objectID": "ip-layer-subnetting.html#summary-of-vln03-slides-3945",
    "href": "ip-layer-subnetting.html#summary-of-vln03-slides-3945",
    "title": "5  IP Layer & Subnetting",
    "section": "Summary of VLN03 — Slides 39–45",
    "text": "Summary of VLN03 — Slides 39–45\n(Routing, Forwarding, and Prefix-Based Tables)\n\n\n1. Forwarding vs Routing (Slides 39–40)\n\nForwarding\n\nA router’s core task: inspect the destination IP address of a packet and forward it to the correct output interface.\nDecision is based on a forwarding table (or FIB), which contains rules for which destination addresses are reachable via which output port.\nThe process is local and repeated per-packet.\n\n\n\nRouting\n\nRefers to the global algorithmic process of building and maintaining forwarding tables.\nRouting protocols (e.g., RIP, OSPF, BGP) distribute topology information so that each router can compute optimal next-hop decisions.\nForwarding is where to send it; routing is how routers learn where to send it.\n\n\n\n\n\n2. From Address Ranges to Prefix Matching (Slides 41–45)\n\nMotivation for compact representation\n\nEarly forwarding tables stored IP ranges like 200.23.16.0 – 200.23.23.255 with associated output ports.\nWhile correct, this format is:\n\nMemory-inefficient\nSlow to search in large tables\nDifficult to aggregate\n\n\n\n\nTransition to prefix-based representation\n\nObserved: all addresses in a range often share a common bit prefix.\nIdea: use CIDR-style prefixes to represent ranges compactly.\n\ne.g., 200.23.16.0 – 200.23.23.255 → 200.23.16.0/21\ne.g., 200.23.24.0 – 200.23.24.255 → 200.23.24.0/24\n\n\n\n\nBinary prefix matching\n\nIn binary: prefixes like 11001000 00010111 00010***** encode address ranges efficiently.\nRouters only compare the relevant prefix bits and ignore the remaining bits (*).\nReduces memory usage and speeds up lookup.\n\n\n\nFinal form: longest-prefix matching table\n\nThe router matches the longest applicable prefix from the table.\nExample table:\n\n\n\nPrefix (CIDR)\nInterface\n\n\n\n\n200.23.16.0/21\n0\n\n\n200.23.24.0/24\n1\n\n\n200.23.25.0/21\n2\n\n\nDefault route\n3\n\n\n\nIf multiple entries match a destination, the one with the most bits in the prefix is selected.\n\n\n\n\n\n3. Benefits of Prefix-Based Forwarding\n\nMemory efficiency: fewer entries needed for large address blocks\nPerformance: supports fast lookup structures (tries, binary trees, etc.)\nScalability: enables route aggregation, reducing table size across the Internet\nFlexibility: can express fine-grained (specific /24) and coarse-grained (aggregated /21) routes",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IP Layer & Subnetting</span>"
    ]
  }
]