[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Operating Systems and Networks SoSe 25 Notes",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "process.html",
    "href": "process.html",
    "title": "1  Process Management",
    "section": "",
    "text": "Condition Variables and Producer / Consumer Problem\nCondition variables are employed together with mutexes when synchronizing producers and consumers. It woul be incorrect ot only use a condition variable without a mutex, or a mutex with busy waiting without a condition varible.",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "process.html#condition-variables-and-producer-consumer-problem",
    "href": "process.html#condition-variables-and-producer-consumer-problem",
    "title": "1  Process Management",
    "section": "",
    "text": "Incorrect Variant 1: Condition Variable Without Mutex\nready = False\ncondition = ConditionVariable()\n\ndef wait_thread():\n    if not ready:\n        condition.wait()  # Incorrect: no mutex guarding shared state\n    print(\"Condition met!\")\n\ndef signal_thread():\n    ready = True\n    condition.notify()\nWhy It’s Wrong:\n\nAccess to ready is unprotected — race conditions may occur.\ncondition.wait() must always be used with a mutex.\n\n\n\nIncorrect Variant 2: Mutex Without Condition Variable (Busy Waiting)\nready = False\nmutex = Mutex()\n\ndef wait_thread():\n    while True:\n        mutex.lock()\n        if ready:\n            mutex.unlock()\n            break\n        mutex.unlock()\n        sleep(0.01)  # Active polling (wasteful)\n\ndef signal_thread():\n    mutex.lock()\n    ready = True\n    mutex.unlock()\nWhy It’s Problematic:\n\nAvoids races, but wastes CPU via busy waiting.\nAlso prone to subtle visibility issues if memory barriers aren’t enforced.\n\n\n\nCorrect Variant: Condition Variable with Mutex\nready = False\nmutex = Mutex()\ncondition = ConditionVariable()\n\ndef wait_thread():\n    mutex.lock()\n    while not ready:\n        condition.wait(mutex)  # Atomically unlocks and waits\n    mutex.unlock()\n    print(\"Condition met!\")\n\ndef signal_thread():\n    mutex.lock()\n    ready = True\n    condition.notify()\n    mutex.unlock()\nWhy It Works:\n\nShared state is properly guarded.\nNo busy waiting.\nSafe signaling and waking.\n\nAnother question is why to use while not ready and not simply if not ready:\ndef wait_thread():\n    mutex.lock()\n    if not ready:\n        condition.wait(mutex)\n    mutex.unlock()\nProblem:\n\nMay miss spurious wakeups or situations where multiple threads wait and only one should proceed.\nA while loop is necessary to recheck the condition after being woken up.\n\n\n\n\nProducer/Consumer Problem\n\nVariant A: Unbounded Queue (No Buffer Limit)\nqueue = []\nmutex = Mutex()\nnot_empty = ConditionVariable()\n\ndef producer():\n    while True:\n        item = produce()\n        mutex.lock()\n        queue.append(item)\n        not_empty.notify()\n        mutex.unlock()\n\ndef consumer():\n    while True:\n        mutex.lock()\n        while not queue:\n            not_empty.wait(mutex)\n        item = queue.pop(0)\n        mutex.unlock()\n        consume(item)\n\n\n\nVariant B: Bounded Queue (Fixed Buffer Size)\nqueue = []\nBUFFER_SIZE = 10\nmutex = Mutex()\nnot_empty = ConditionVariable()\nnot_full = ConditionVariable()\n\ndef producer():\n    while True:\n        item = produce()\n        mutex.lock()\n        while len(queue) &gt;= BUFFER_SIZE:\n            not_full.wait(mutex)\n        queue.append(item)\n        not_empty.notify()\n        mutex.unlock()\n\ndef consumer():\n    while True:\n        mutex.lock()\n        while not queue:\n            not_empty.wait(mutex)\n        item = queue.pop(0)\n        not_full.notify()\n        mutex.unlock()\n        consume(item)",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "process.html#summary-table",
    "href": "process.html#summary-table",
    "title": "1  Process Management",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\n\n\nCase\nUses Mutex\nUses Condition Variable\nBlocking\nCPU-Efficient\nCorrect\n\n\n\n\n1. Condition variable without mutex\nNo\nYes\nNo\nYes\nNo\n\n\n2. Mutex without condition variable\nYes\nNo\nNo\nNo (busy)\nPartly\n\n\n3. Condition variable with mutex\nYes\nYes\nYes\nYes\nYes\n\n\n4. If instead of while\nYes\nYes\nYes\nYes\nRisky\n\n\n5. Producer/Consumer (unbounded)\nYes\nYes (not_empty)\nYes\nYes\nYes\n\n\n6. Producer/Consumer (bounded)\nYes\nYes (not_empty, not_full)\nYes\nYes\nYes\n\n\n\n\n\nOperations of a Bounded Queue\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nin\nout\nBuffer State\nCount == ((in - out + 5) % 5)\n\n\n\n\n0\nStart\n0\n0\n[_ _ _ _ _]\n0\n\n\n1\nProduce A\n1\n0\n[A _ _ _ _]\n1\n\n\n2\nProduce B\n2\n0\n[A B _ _ _]\n2\n\n\n3\nProduce C\n3\n0\n[A B C _ _]\n3\n\n\n4\nConsume → A\n3\n1\n[_ B C _ _]\n2\n\n\n5\nConsume → B\n3\n2\n[_ _ C _ _]\n1\n\n\n6\nProduce D\n4\n2\n[_ _ C D _]\n2\n\n\n7\nProduce E\n0\n2\n[_ _ C D E]\n3\n\n\n8\nConsume → C\n0\n3\n[_ _ _ D E]\n2\n\n\n9\nProduce F\n1\n3\n[F _ _ D E]\n3\n\n\n\nwhere\n\nin: the write position / index\nout: the read position /index\ncount == (in - out + 5) % 5 is the invariant of the data structure, giving the number of elements in the buffer",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "memory.html",
    "href": "memory.html",
    "title": "2  Memory Management",
    "section": "",
    "text": "Virtual Memory",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Memory Management</span>"
    ]
  },
  {
    "objectID": "memory.html#virtual-memory",
    "href": "memory.html#virtual-memory",
    "title": "2  Memory Management",
    "section": "",
    "text": "Paging\n\nTranslating Logical to Physical Addresses\n\nContext\nIn paging, the operating system divides:\n\nLogical (virtual) memory into fixed-size pages\nPhysical memory (RAM) into same-size frames\n\nEach process has a page table that maps page numbers to frame numbers.\nOur goal is:\n\nGiven a virtual address, compute the corresponding physical address.\n\n\n\nExample Setup\n\nVirtual address \\(V = 7000\\)\nPage size = 4096 bytes = \\(2^{12}\\) ⇒ \\(k = 12\\)\nAssume the page table maps page 1 to frame 9: \\(F(1) = 9\\)\n\n\n\nStep 1: Manual (Arithmetic) Calculation\nTo translate a virtual address manually, we need to answer two questions:\n\nWhich page is the address in?\nWhere within that page is the address?\n\nThis is done by:\n\nDividing the address by the page size to get the page number\nTaking the remainder (modulo) to get the offset within the page\n\nApply this to \\(V = 7000\\) with page size 4096:\n\nPage number \\(p = \\left\\lfloor \\frac{7000}{4096} \\right\\rfloor = 1\\)\nOffset \\(d = 7000 \\mod 4096 = 2904\\)\n\nNow we look up page 1 in the page table:\n\nFrame number \\(f = F(1) = 9\\)\n\nTo get the final physical address, we compute the base address of frame 9 and add the offset:\n\nPhysical address = \\(f \\cdot 4096 + d = 9 \\cdot 4096 + 2904 = 39768\\)\n\nResult: 39768\n\n\nStep 2: Bitwise Calculation (Optimized for Hardware)\nFor power-of-two page sizes, the address can be efficiently split using bitwise operations:\n\nPage number = \\(V \\gg 12\\) (right shift by 12 bits is equivalent to dividing by 4096)\nOffset = \\(V \\& (2^{12} - 1) = V \\& 0xFFF\\) (bit mask keeps the lower 12 bits)\nPage number 1 maps to frame number \\(f = F(1) = 9\\)\n\nTo compute the frame’s starting address, we use a left shift:\n\n\\(f \\ll 12 = 9 \\ll 12 = 36864\\), which is equivalent to \\(9 \\cdot 4096\\)\n\nFinal physical address:\n\n\\(\\text{Physical Address} = 36864 + 2904 = 39768\\)\n\nSame result, now using fast bit operations.\n\n\nBit Sequence Visualization\nLet’s visualize how the virtual address is split in binary:\n\nVirtual address \\(V = 7000\\)\nBinary representation (14 bits): 0001 1011 0101 1000\n\nSplit into:\n\nPage number (upper 2 bits): 00 01 → 1\nOffset (lower 12 bits): 1011 0101 1000 → 2904\n\nThis split works because:\n\nThe lower 12 bits represent the offset for a 4 KB page\nThe upper bits index into the page table\n\n\n\nWhy This Works Mathematically\nThe logic behind using bit shifts and masks instead of division and modulo is based on how numbers are represented in binary.\n\nDecimal Analogy (Base 10)\nConsider dividing 1375 by powers of 10:\n\n1375 ÷ 10¹ = 137 (modulo: 5)\n1375 ÷ 10² = 13 (modulo: 75)\n1375 ÷ 10³ = 1 (modulo: 375)\n\nThe rightmost digits are the remainder (modulo); the left are the quotient (division).\n\n\nBinary Example (Base 2)\nTake the binary number 1011₂ (= 11₁₀):\n\n1011 ÷ 2¹ = 101 = 5 (modulo: 1)\n1011 ÷ 2² = 10 = 2 (modulo: 11 = 3)\n1011 ÷ 2³ = 1 = 1 (modulo: 011 = 3)\n\nIn both systems, the rightmost digits/bits represent the offset, and the leftmost represent the page number.\nThis is why in binary:\n\n\\(V \\gg k\\) is equivalent to \\(\\left\\lfloor V / 2^k \\right\\rfloor\\)\n\\(V \\& (2^k - 1)\\) is equivalent to \\(V \\mod 2^k\\)\n\\(f \\ll k\\) is equivalent to \\(f \\cdot 2^k\\), which gives the frame base address\n\nThese operations are both mathematically correct and hardware-efficient.\n\n\n\nFinal Formula\n\\[\n\\text{Physical Address} = \\left( F(V \\gg k) \\ll k \\right) + \\left( V \\& (2^k - 1) \\right)\n\\]\nThis computes:\n\nThe page number via right shift\nThe frame number from the page table\nThe frame base via left shift (i.e., multiplying by page size)\nThe final physical address by adding the offset\n\n\n\nAdditional Example for Practice and Clarity\nLet’s now take another address and apply all three methods for reinforcement.\n\nSetup\n\nVirtual address \\(V = 13,\\!452\\)\nPage size = 4096 = \\(2^{12}\\)\nPage table:\n\n\n\n\nPage #\nFrame #\n\n\n\n\n0\n3\n\n\n1\n7\n\n\n2\n1\n\n\n3\n6\n\n\n\n\n\nManual Calculation\n\nPage number: \\(13,\\!452 \\div 4096 = 3\\)\nOffset: \\(13,\\!452 \\mod 4096 = 1164\\)\nFrame number: \\(F(3) = 6\\)\nPhysical address = \\(6 \\cdot 4096 + 1164 = 24,\\!576 + 1164 = 25,\\!740\\)\n\n\n\nBitwise Calculation\n\n\\(V = 13,\\!452 = 0b0011\\ 0100\\ 1001\\ 1100\\)\nPage number = \\(V \\gg 12 = 3\\)\nOffset = \\(V \\& 0xFFF = 1164\\)\nFrame number = \\(F(3) = 6\\)\nFrame base = \\(6 \\ll 12 = 24,\\!576\\)\nPhysical address = \\(24,\\!576 + 1164 = 25,\\!740\\)\n\n\n\n\nUsing the Formula\n\\[\n\\text{Physical Address} = (F(V \\gg 12) \\ll 12) + (V \\& 0xFFF)\n\\]\n\\[\n= (6 \\ll 12) + 1164 = 24,\\!576 + 1164 = 25,\\!740\n\\]\n\n\nConclusion\nWhen the page size is a power of two, address translation can be performed using fast bit operations instead of division and modulo. This is possible because of how binary numbers encode positional value. We saw that the lower bits give the offset and the upper bits the page number. Whether done manually, with bit operations, or using the translation formula, all approaches yield the same physical address — and this consistency is what makes paging both robust and efficient.\nAbsolutely — here is the complete regenerated summary, now incorporating:\n\nThe updated “Inverted Page Tables” section with size explanation and example\nThe updated “Hierarchical Page Tables” section with both the 32-bit and 64-bit address resolution examples and definitions of each table level\nConsistent formatting throughout:\n\nNo bold in headers\nMinimal boldface emphasis in the text — used only where strictly useful for clarity\n\n\nThis version is fully ready for integration into your Quarto notes.\n\n\n\n\nPage Tables\n\nSingle-Level (Direct) Page Tables\nIn the simplest form of paging, each process has its own single-level page table, which directly maps virtual page numbers to physical frame numbers.\nFor example, in a system with:\n\nA 32-bit virtual address space (4 GB total)\nA page size of 4 KB = \\(2^{12}\\) bytes\n\nThe number of virtual pages is:\n\\[\n2^{32} / 2^{12} = 2^{20} = 1,\\!048,\\!576 \\text{ entries}\n\\]\nIf each page table entry (PTE) is 4 bytes, the total size of the page table is:\n\\[\n2^{20} \\times 4 = 4 \\text{ MB per process}\n\\]\nIn a 64-bit system, even with larger pages (e.g. 4 MB), the number of virtual pages is so large (e.g., \\(2^{52}\\)) that flat page tables become completely impractical.\n\n\n\nWhy Single-Level Tables Are Impractical\nMain issues:\n\nMemory usage per process becomes excessive (e.g., 4 MB/page table × hundreds of processes)\nScaling issues as address spaces grow\nMost processes use only a small part of their virtual address space, so allocating full page tables is wasteful\n\nThus, alternative paging strategies are needed.\n\n\n\nFrame Table (Global Physical Memory Tracking)\nThe OS maintains a global frame table, which tracks:\n\nWhich physical frames are in use\nWhat each frame is used for (user page, kernel structure, page table, etc.)\nAssociated metadata: dirty bit, reference count, owner process\n\nThis allows the OS to allocate and deallocate physical memory intelligently and safely. It is also crucial for page replacement algorithms, memory protection, and managing shared pages or I/O buffers.\n\n\n\nInverted Page Tables\nIn a traditional page table system, each process maintains its own page table, which maps virtual pages to physical frames. In contrast, an inverted page table uses a fundamentally different approach:\n\nThere is a single global page table for the entire system\nIt contains one entry for each physical frame, not for each virtual page\nEach entry records:\n\nThe process ID that owns the frame\nThe virtual page number that maps to it\nAny additional metadata (e.g., access flags, validity)\n\n\nThis approach dramatically reduces memory overhead, especially in systems with large virtual address spaces.\n\nSize of the inverted page table\nThe size of an inverted page table depends only on the number of physical frames, which is determined by:\n\\[\n\\text{Number of entries} = \\frac{\\text{RAM size}}{\\text{frame size}}\n\\]\nEach entry stores fixed-size metadata (such as PID and VPN), so the total size is:\n\\[\n\\text{Table size} = \\frac{\\text{RAM size}}{\\text{frame size}} \\times \\text{entry size}\n\\]\nThe ratio of the table size to RAM size simplifies to:\n\\[\n\\frac{\\text{Table size}}{\\text{RAM size}} = \\frac{\\text{entry size}}{\\text{frame size}}\n\\]\nThis ratio is independent of total RAM size. In other words, the memory overhead of the page table scales proportionally with RAM but is bounded by the frame size and the entry size.\n\n\nConcrete example\nConsider a 32-bit system with the following properties:\n\nPhysical RAM: 4 GB = \\(2^{32}\\) bytes\nPage/frame size: 4 KB = \\(2^{12}\\) bytes\nPage table entry size: 8 bytes (to store PID, VPN, flags, etc.)\n\nStep-by-step:\n\nNumber of physical frames:\n\n\\[\n\\frac{2^{32}}{2^{12}} = 2^{20} = 1,\\!048,\\!576 \\text{ frames}\n\\]\n\nTotal inverted page table size:\n\n\\[\n2^{20} \\times 8 = 8 \\text{ MB}\n\\]\n\nRelative overhead:\n\n\\[\n\\frac{8 \\text{ MB}}{4 \\text{ GB}} = \\frac{1}{512}\n\\]\nThis means the page table occupies only about 0.2% of RAM.\n\n\nSummary of trade-offs\nInverted page tables offer substantial memory savings, especially on systems with large or sparsely used virtual address spaces. However, the downside is that address translation becomes more complex:\n\nThe system must search (or hash) the page table to find the matching (process ID, virtual page) pair\nThis lookup is slower than direct indexing\nTLB caching becomes less straightforward\n\nFor this reason, inverted page tables are rarely used in modern general-purpose OSes, though they are still valuable in embedded or resource-constrained systems.\n\n\n\n\nHierarchical Page Tables\nModern systems (e.g., x86, Linux, Windows) use multi-level page tables to avoid allocating massive single-level tables for sparse address spaces. The key idea is to divide the virtual address into multiple segments, each of which indexes a level in the page table hierarchy. This allows the OS to only allocate memory for regions that are actually used.\nEach level of the hierarchy resolves part of the virtual address and points to the next level down. The final level contains the physical frame number. The remaining bits (the offset) are added to form the final physical address.\nThis approach reduces memory overhead and supports sparse, large virtual address spaces.\n\n32-bit Two-Level Paging Example\nAssume a 32-bit virtual address space with:\n\nPage size = 4 KB = \\(2^{12}\\)\n10 bits for the page directory index\n10 bits for the page table index\n12 bits for the offset\n\nThe virtual address layout is:\n[ 10 bits | 10 bits | 12 bits ]\n   PD index  PT index   Offset\nSuppose the virtual address is:\nVA = 0x1234ABCD\nConvert to binary:\n0001 0010 0011 0100 1010 1011 1100 1101\nSplit:\n\nPage Directory index = 0001001000 = 0x048 = 72\nPage Table index = 1101001010 = 0x34A = 842\nOffset = 101111001101 = 0xBCD = 3021\n\nAssume:\n\nThe page directory is located at physical address 0x00100000\nEntry 72 in the page directory points to a page table at 0x00200000\nEntry 842 in that page table points to a physical frame at 0x00ABC000\n\nFinal physical address:\n\\[\n0x00ABC000 + 0xBCD = 0x00ABCBCD\n\\]\nThis example illustrates how a 2-level table hierarchy resolves the virtual address to a physical address through two indirections and an offset.\n\n\n64-bit Four-Level Paging Example\nModern x86-64 systems typically support a 48-bit virtual address space, split across four paging levels. The page size remains 4 KB = \\(2^{12}\\).\nEach level of the hierarchy resolves 9 bits (since \\(2^9 = 512\\) entries per table), so the full 48-bit address is broken into:\n[ 9 bits | 9 bits | 9 bits | 9 bits | 12 bits ]\n  PML4     PDPT     PD       PT       Offset\nThe levels are defined as follows:\n\nPML4 (Page Map Level 4): The root of the page table hierarchy; indexed by the top 9 bits of the virtual address. Each entry points to a PDPT.\nPDPT (Page Directory Pointer Table): Intermediate level; each entry points to a Page Directory.\nPD (Page Directory): Each entry points to a Page Table.\nPT (Page Table): Final level; each entry contains a physical frame number.\nOffset: Specifies the exact byte within the 4 KB page.\n\nSuppose the virtual address is:\nVA = 0x00007F34_1234ABCD\nBreaking down the lower 48 bits:\n\nPML4 index = bits 47–39 = 0\nPDPT index = bits 38–30 = 505\nPD index = bits 29–21 = 322\nPT index = bits 20–12 = 210\nOffset = bits 11–0 = 0xAF3D = 44861\n\nAssume the following physical mappings:\n\nCR3 register points to PML4 at 0x00100000\nPML4 entry 0 → PDPT at 0x00200000\nPDPT entry 505 → PD at 0x00300000\nPD entry 322 → PT at 0x00400000\nPT entry 210 → frame at 0x00ABC000\n\nFinal physical address:\n\\[\n0x00ABC000 + 0xAF3D = 0x00B06F3D\n\\]\nThis example demonstrates how a virtual address is translated step-by-step through four levels of indirection. The layered structure supports extremely large address spaces (up to 256 TB) without requiring full allocation of all intermediate tables.\n\n\nSummary\nHierarchical page tables solve the scalability problem of flat page tables by breaking the virtual address into multiple segments. Each level of the hierarchy is a smaller table, and lower levels are allocated only when needed. This provides a sparse, memory-efficient structure for address translation.\nThe cost of additional indirection is mitigated by the use of TLBs, which cache recent address translations to avoid repeated page walks.\n\n\n\n\nWhen and How Page Tables Are Allocated\nPage tables are allocated in two situations:\n\nAt program load time The OS allocates top-level tables and reserves virtual address regions for code, data, stack, etc., but not necessarily all intermediate tables.\nOn demand via page faults When a process accesses a virtual address with no current mapping, the CPU triggers a page fault. If the access is valid, the OS allocates missing intermediate page tables and a physical frame, updates the page table entries, and resumes execution.\n\nThis approach enables sparse memory allocation and efficient use of physical memory.\n\n\n\nPhysical Memory: Frames and Their Usage\nRAM is divided into fixed-size frames (e.g., 4 KB). Each frame can hold:\n\nA user page (code, stack, heap)\nA page table (of any level)\nA kernel structure\nOther memory-resident objects\n\nContiguous physical frames may contain completely unrelated contents, as physical memory management is modular and page-based. The OS tracks frame usage via the global frame table.\n\n\n\nKernel Mapping and Access\nThe kernel is mapped into the upper region of each process’s virtual address space (e.g., from 0xC0000000 upward in 32-bit systems). This allows:\n\nFast system calls and interrupt handling\nAvoiding page table switches on mode transitions\n\nThis region is protected by page table flags, preventing access in user mode. The kernel itself runs entirely in kernel mode. Its physical location is determined at boot and may vary across systems. Techniques like KASLR (Kernel Address Space Layout Randomization) add further protection.\n\n\n\nTranslation Lookaside Buffer\nThe TLB is a hardware-managed cache used by the MMU to store recently used virtual-to-physical page translations.\nWhy it’s important:\n\nPage walks involve multiple memory accesses\nThe TLB allows near-instant translation on a hit\nReduces the average cost of memory access in the presence of multi-level page tables\n\nTLBs are small (typically 16–512 entries) but highly effective due to temporal and spatial locality in most program behavior.",
    "crumbs": [
      "Operating Systems",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Memory Management</span>"
    ]
  },
  {
    "objectID": "networks-intro.html",
    "href": "networks-intro.html",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#overview",
    "href": "networks-intro.html#overview",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "",
    "text": "Topic\nSlide Range\nNotes\n\n\n\n\n1. History & Fundamentals of the Internet\nSlides 1–16\nARPANET, TCP/IP, WWW, client-server, HTTP statelessness\n\n\n2. Circuit vs. Packet Switching\nSlides 17–23\nFDM, TDM, statistical multiplexing, delay tradeoffs\n\n\n3. Delays, Loss, Throughput\nSlides 24–35\nd_proc, d_queue, La/R, traceroute, throughput bottlenecks\n\n\n4. Protocol Layers and Encapsulation\nSlides 36–44\nLayer model, encapsulation, host/router/switch roles",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#historical-background-and-internet-foundations-slides-116",
    "href": "networks-intro.html#historical-background-and-internet-foundations-slides-116",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Historical Background and Internet Foundations (Slides 1–16)",
    "text": "Historical Background and Internet Foundations (Slides 1–16)\n\nEarly developments: ARPANET, Cyclades, and ALOHANet pioneered packet switching.\nThe Internet emerged as a global interconnection of autonomous systems using the TCP/IP protocol suite.\nTim Berners-Lee’s World Wide Web (1989–1991) introduced:\n\nA unified model of hyperlinked documents (HTML)\nThe HTTP protocol (stateless)\nURLs for addressing\nThe browser-server interaction model\n\nThe stateless nature of HTTP means each request is handled independently — servers do not retain memory of previous interactions.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#transmission-media-and-infrastructure-slides-1620",
    "href": "networks-intro.html#transmission-media-and-infrastructure-slides-1620",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Transmission Media and Infrastructure (Slides 16–20)",
    "text": "Transmission Media and Infrastructure (Slides 16–20)\n\nData transmission can occur over:\n\nCopper (UTP): electrical signals\nFiber optics: light pulses\nRadio: electromagnetic waves (Wi-Fi, LTE)\n\nFiber-optic links offer high bandwidth and low latency — widely used in backbone and undersea cables.\nSatellite communication has higher propagation delay (≥500 ms round-trip) due to distance (~36,000 km geostationary orbit).\nReal-world systems combine many media and technologies in layered infrastructure.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#circuit-switching-vs.-packet-switching-slides-1823",
    "href": "networks-intro.html#circuit-switching-vs.-packet-switching-slides-1823",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Circuit Switching vs. Packet Switching (Slides 18–23)",
    "text": "Circuit Switching vs. Packet Switching (Slides 18–23)\n\nCircuit switching: fixed, reserved paths (e.g. telephony)\n\nUses TDM (Time Division Multiplexing) or FDM (Frequency Division Multiplexing)\n\nPacket switching: data is broken into packets routed independently\n\nUses statistical multiplexing\nNo reservation of bandwidth; packets share the link dynamically\n\nTrade-offs of packet switching:\n\nMore efficient use of bandwidth under bursty traffic\nPotential for packet delay, loss, and reordering",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#network-performance-metrics-slides-2432",
    "href": "networks-intro.html#network-performance-metrics-slides-2432",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Network Performance Metrics (Slides 24–32)",
    "text": "Network Performance Metrics (Slides 24–32)\n\nFour types of delay:\n\nProcessing delay: time to examine packet header and perform checks\nQueueing delay: time waiting in the router buffer\nTransmission delay:\n\\[\nd_{\\text{trans}} = \\frac{L}{R}\n\\]\nwhere:\n\n\\(L\\): packet size (bits)\n\\(R\\): link bandwidth (bps)\n\nPropagation delay:\n\\[\nd_{\\text{prop}} = \\frac{d}{s}\n\\]\nwhere:\n\n\\(d\\): physical distance (meters)\n\\(s\\): signal propagation speed (m/s)\n\n\n\n\nTotal node delay:\n\\[\nd_{\\text{nodal}} = d_{\\text{proc}} + d_{\\text{queue}} + d_{\\text{trans}} + d_{\\text{prop}}\n\\]\n\n\nTraffic intensity and queue behavior:\nLet:\n\n\\(a\\): average packet arrival rate (packets/sec)\n\\(L\\): packet size (bits)\n\\(R\\): link bandwidth (bps)\n\nThen:\n\\[\n\\text{Traffic intensity} = \\frac{aL}{R}\n\\]\nInterpretation:\n\nIf \\(\\frac{aL}{R} \\geq 1\\): the queue grows without bound\nAs \\(\\frac{aL}{R} \\rightarrow 1\\): delay increases sharply\n\n\n\nEnd-to-end delay over multiple hops:\n\\[\nd_{\\text{end-to-end}} = \\sum_{i=1}^N (d_{\\text{proc},i} + d_{\\text{queue},i} + d_{\\text{trans},i} + d_{\\text{prop},i})\n\\]\nwhere \\(N\\) is the number of routers.\n\n\nTraceroute:\n\nUses IP TTL (Time-To-Live) field to probe each hop\nWhen TTL reaches zero, routers send an ICMP “Time Exceeded” message\nAllows measurement of round-trip time (RTT) per hop",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#throughput-slides-3335",
    "href": "networks-intro.html#throughput-slides-3335",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Throughput (Slides 33–35)",
    "text": "Throughput (Slides 33–35)\n\nThroughput: the rate at which data is successfully delivered (bps)\n\n\nTwo cases (Slide 34):\nIf:\n\n\\(R_S\\): server’s sending rate\n\\(R_C\\): client-side link rate\n\nThen:\n\nIf \\(R_S &lt; R_C\\), then \\(\\text{Throughput} = R_S\\)\nIf \\(R_S &gt; R_C\\), then \\(\\text{Throughput} = R_C\\)\n\n\\[\n\\text{Throughput} = \\min(R_S, R_C)\n\\]\n\n\nMulti-user sharing (Slide 35):\nIf 10 users share a backbone link of rate \\(R\\), and each has:\n\nSender link: \\(R_s\\)\nReceiver link: \\(R_c\\)\n\nThen per-connection throughput is:\n\\[\n\\text{Throughput} = \\min(R_s, R_c, \\frac{R}{10})\n\\]\n\nYes — definitely. The current version is clean and comprehensive, but if your goal is clarity + conciseness for study purposes, we can streamline it without sacrificing completeness.\nHere is a more succinct version of the same summary, optimized for use in study notes:",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#summary-protokollschichten-und-ihre-dienstmodelle-slides-36---45",
    "href": "networks-intro.html#summary-protokollschichten-und-ihre-dienstmodelle-slides-36---45",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Summary: Protokollschichten und ihre Dienstmodelle (Slides 36 - 45)",
    "text": "Summary: Protokollschichten und ihre Dienstmodelle (Slides 36 - 45)\n(Slides 36–44)\nThis final chapter introduces the layered architecture of the Internet. It explains how each protocol layer serves the one above and relies on the one below, and how encapsulation enables structured communication.\n\n\nLayering Motivation (Slide 36–37)\n\nNetworks are complex (hosts, routers, media, apps).\nSolution: Schichtenarchitektur for modular design.\nEach layer performs actions and uses only the services of the layer below.\n\n\n\n\nProtocol Layering: Foundations (Slides 36–38)\n\nDue to network complexity, functionality is divided into layers, each with clear responsibilities.\nA layer \\(k\\) uses only the services of layer \\(k-1\\):\n\\[\n\\text{Layer } k \\longrightarrow \\text{uses services of Layer } (k-1)\n\\]\nEach layer communicates vertically (service interface) and defines horizontal protocols (with its counterpart on the remote host).\nLayering enables:\n\nModularity\nReplaceability\nInteroperability\nAbstraction from hardware details\n\n\n\n\nThe Internet Stack (Slide 38–39)\n\n\n\n\n\n\n\n\nLayer\nFunction\nExamples\n\n\n\n\nApplication\nApplication protocols, user data\nHTTP, FTP, SMTP\n\n\nTransport\nProcess-to-process delivery\nTCP, UDP\n\n\nNetwork\nHost-to-host delivery, routing\nIP, ICMP\n\n\nData Link\nFrame-level delivery on local links\nEthernet, Wi-Fi, PPP\n\n\nPhysical\nTransmission of bits over the medium\nFiber, DSL, 5G\n\n\n\n\nLayers are identified by who communicates (e.g. processes, hosts, links).\nData is encapsulated step by step as it moves downward.\n\n\n\n\nProtocol Scope by Device (Slide 40)\n\n\n\nDevice\nImplements Up To\n\n\n\n\nHost\nAll 5 layers\n\n\nRouter\nNetwork layer (IP)\n\n\nSwitch\nData Link layer (MAC)\n\n\n\n\n\n\nEncapsulation (Slides 41–43)\nEach layer adds its own header (and possibly trailer). The result:\nFrame = [Data Link hdr] + [IP hdr] + [TCP hdr] + Message + [Trailer]\nAt the receiver, each layer removes its own header.\n\nOnly hosts process all layers.\nRouters read only IP headers.\nSwitches forward based on MAC addresses.\n\n\n\nOSI Model (Slide 44)\nA 7-layer reference model defined by ISO, used mostly for conceptual clarity.\n\n\n\nOSI Layer\nAdded vs. Internet Model\n\n\n\n\n7: Application\nMatches Internet’s application layer\n\n\n6: Presentation\nData format, compression, encryption\n\n\n5: Session\nDialog management\n\n\n4–1: Transport → Physical\nSame as in Internet stack\n\n\n\n\nInternet model simplifies OSI: layers 5–7 are often merged into the application.\n\n\nThis version keeps all major points but trims redundant explanation and tightens the wording for effective study reference.\nWould you like it exported to markdown or added to your ongoing Quarto notes?",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  },
  {
    "objectID": "networks-intro.html#unified-protocoll-stack-overview",
    "href": "networks-intro.html#unified-protocoll-stack-overview",
    "title": "3  Network Fundamentals - Summary of Slides 1 - 45",
    "section": "Unified Protocoll Stack Overview",
    "text": "Unified Protocoll Stack Overview\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nCommunication Endpoint\nData Unit Name\nWhat It Contains\nAdds Header/Footer?\nCan Split Data?\nTypical Protocols\n\n\n\n\nApplication\nApplications or processes (e.g., browser ↔︎ web server)\nMessage\nApp-level data (e.g. HTTP, SMTP)\nNo\nYes — application logic (e.g. file chunks)\nHTTP, FTP, SMTP, DNS, TLS, SSH, POP, IMAP\n\n\nTransport (TCP/UDP)\nSockets on end hosts (process ↔︎ process)\nSegment\nMessage + TCP/UDP header\nYes — transport header\nYes — TCP segments long messages\nTCP, UDP\n\n\nNetwork (IP)\nHosts or end systems (host ↔︎ host, abstracting from processes)\nPacket (or Datagram)\nSegment + IP header\nYes — network header\nYes — IP may fragment large packets\nIP (v4/v6), ICMP, IGMP\n\n\nData Link\nDirectly connected devices (e.g. Host ↔︎ Router)\nFrame\nPacket + MAC header + trailer (e.g. CRC)\nYes — frame header and trailer\nNo — one packet per frame\nEthernet, Wi-Fi (802.11), PPP, ARP\n\n\nPhysical\nPhysical interfaces (e.g., NICs, cables, radio) exchanging raw bits\nBits\nEncoded electrical/optical/radio signals\nN/A (not in software)\nNo — transmits one bit at a time\nDSL, Optical Fiber, Ethernet Cable, 5G",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Network Fundamentals - Summary of Slides 1 - 45</span>"
    ]
  }
]