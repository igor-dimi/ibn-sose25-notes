[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Operating Systems and Networks SoSe 25 Notes",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "process.html",
    "href": "process.html",
    "title": "1  Process Management",
    "section": "",
    "text": "Condition Variables and Producer / Consumer Problem\nCondition variables are employed together with mutexes when synchronizing producers and consumers. It woul be incorrect ot only use a condition variable without a mutex, or a mutex with busy waiting without a condition varible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "process.html#condition-variables-and-producer-consumer-problem",
    "href": "process.html#condition-variables-and-producer-consumer-problem",
    "title": "1  Process Management",
    "section": "",
    "text": "Incorrect Variant 1: Condition Variable Without Mutex\nready = False\ncondition = ConditionVariable()\n\ndef wait_thread():\n    if not ready:\n        condition.wait()  # Incorrect: no mutex guarding shared state\n    print(\"Condition met!\")\n\ndef signal_thread():\n    ready = True\n    condition.notify()\nWhy It’s Wrong:\n\nAccess to ready is unprotected — race conditions may occur.\ncondition.wait() must always be used with a mutex.\n\n\n\nIncorrect Variant 2: Mutex Without Condition Variable (Busy Waiting)\nready = False\nmutex = Mutex()\n\ndef wait_thread():\n    while True:\n        mutex.lock()\n        if ready:\n            mutex.unlock()\n            break\n        mutex.unlock()\n        sleep(0.01)  # Active polling (wasteful)\n\ndef signal_thread():\n    mutex.lock()\n    ready = True\n    mutex.unlock()\nWhy It’s Problematic:\n\nAvoids races, but wastes CPU via busy waiting.\nAlso prone to subtle visibility issues if memory barriers aren’t enforced.\n\n\n\nCorrect Variant: Condition Variable with Mutex\nready = False\nmutex = Mutex()\ncondition = ConditionVariable()\n\ndef wait_thread():\n    mutex.lock()\n    while not ready:\n        condition.wait(mutex)  # Atomically unlocks and waits\n    mutex.unlock()\n    print(\"Condition met!\")\n\ndef signal_thread():\n    mutex.lock()\n    ready = True\n    condition.notify()\n    mutex.unlock()\nWhy It Works:\n\nShared state is properly guarded.\nNo busy waiting.\nSafe signaling and waking.\n\nAnother question is why to use while not ready and not simply if not ready:\ndef wait_thread():\n    mutex.lock()\n    if not ready:\n        condition.wait(mutex)\n    mutex.unlock()\nProblem:\n\nMay miss spurious wakeups or situations where multiple threads wait and only one should proceed.\nA while loop is necessary to recheck the condition after being woken up.\n\n\n\n\nProducer/Consumer Problem\n\nVariant A: Unbounded Queue (No Buffer Limit)\nqueue = []\nmutex = Mutex()\nnot_empty = ConditionVariable()\n\ndef producer():\n    while True:\n        item = produce()\n        mutex.lock()\n        queue.append(item)\n        not_empty.notify()\n        mutex.unlock()\n\ndef consumer():\n    while True:\n        mutex.lock()\n        while not queue:\n            not_empty.wait(mutex)\n        item = queue.pop(0)\n        mutex.unlock()\n        consume(item)\n\n\n\nVariant B: Bounded Queue (Fixed Buffer Size)\nqueue = []\nBUFFER_SIZE = 10\nmutex = Mutex()\nnot_empty = ConditionVariable()\nnot_full = ConditionVariable()\n\ndef producer():\n    while True:\n        item = produce()\n        mutex.lock()\n        while len(queue) &gt;= BUFFER_SIZE:\n            not_full.wait(mutex)\n        queue.append(item)\n        not_empty.notify()\n        mutex.unlock()\n\ndef consumer():\n    while True:\n        mutex.lock()\n        while not queue:\n            not_empty.wait(mutex)\n        item = queue.pop(0)\n        not_full.notify()\n        mutex.unlock()\n        consume(item)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "process.html#summary-table",
    "href": "process.html#summary-table",
    "title": "1  Process Management",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\n\n\nCase\nUses Mutex\nUses Condition Variable\nBlocking\nCPU-Efficient\nCorrect\n\n\n\n\n1. Condition variable without mutex\nNo\nYes\nNo\nYes\nNo\n\n\n2. Mutex without condition variable\nYes\nNo\nNo\nNo (busy)\nPartly\n\n\n3. Condition variable with mutex\nYes\nYes\nYes\nYes\nYes\n\n\n4. If instead of while\nYes\nYes\nYes\nYes\nRisky\n\n\n5. Producer/Consumer (unbounded)\nYes\nYes (not_empty)\nYes\nYes\nYes\n\n\n6. Producer/Consumer (bounded)\nYes\nYes (not_empty, not_full)\nYes\nYes\nYes\n\n\n\n\n\nOperations of a Bounded Queue\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nin\nout\nBuffer State\nCount == ((in - out + 5) % 5)\n\n\n\n\n0\nStart\n0\n0\n[_ _ _ _ _]\n0\n\n\n1\nProduce A\n1\n0\n[A _ _ _ _]\n1\n\n\n2\nProduce B\n2\n0\n[A B _ _ _]\n2\n\n\n3\nProduce C\n3\n0\n[A B C _ _]\n3\n\n\n4\nConsume → A\n3\n1\n[_ B C _ _]\n2\n\n\n5\nConsume → B\n3\n2\n[_ _ C _ _]\n1\n\n\n6\nProduce D\n4\n2\n[_ _ C D _]\n2\n\n\n7\nProduce E\n0\n2\n[_ _ C D E]\n3\n\n\n8\nConsume → C\n0\n3\n[_ _ _ D E]\n2\n\n\n9\nProduce F\n1\n3\n[F _ _ D E]\n3\n\n\n\nwhere\n\nin: the write position / index\nout: the read position /index\ncount == (in - out + 5) % 5 is the invariant of the data structure, giving the number of elements in the buffer",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process Management</span>"
    ]
  },
  {
    "objectID": "memory.html",
    "href": "memory.html",
    "title": "2  Memory Management",
    "section": "",
    "text": "Virtual Memory",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Memory Management</span>"
    ]
  },
  {
    "objectID": "memory.html#virtual-memory",
    "href": "memory.html#virtual-memory",
    "title": "2  Memory Management",
    "section": "",
    "text": "Paging: Translating Logical to Physical Addresses\n\nContext\nIn paging, the operating system divides:\n\nLogical (virtual) memory into fixed-size pages\nPhysical memory (RAM) into same-size frames\n\nEach process has a page table that maps page numbers to frame numbers.\nOur goal is:\n\nGiven a virtual address, compute the corresponding physical address.\n\n\n\nExample Setup\n\nVirtual address \\(V = 7000\\)\nPage size = 4096 bytes = \\(2^{12}\\) ⇒ \\(k = 12\\)\nAssume the page table maps page 1 to frame 9: \\(F(1) = 9\\)\n\n\n\nStep 1: Manual (Arithmetic) Calculation\nTo translate a virtual address manually, we need to answer two questions:\n\nWhich page is the address in?\nWhere within that page is the address?\n\nThis is done by:\n\nDividing the address by the page size to get the page number\nTaking the remainder (modulo) to get the offset within the page\n\nApply this to \\(V = 7000\\) with page size 4096:\n\nPage number \\(p = \\left\\lfloor \\frac{7000}{4096} \\right\\rfloor = 1\\)\nOffset \\(d = 7000 \\mod 4096 = 2904\\)\n\nNow we look up page 1 in the page table:\n\nFrame number \\(f = F(1) = 9\\)\n\nTo get the final physical address, we compute the base address of frame 9 and add the offset:\n\nPhysical address = \\(f \\cdot 4096 + d = 9 \\cdot 4096 + 2904 = 39768\\)\n\nResult: 39768\n\n\nStep 2: Bitwise Calculation (Optimized for Hardware)\nFor power-of-two page sizes, the address can be efficiently split using bitwise operations:\n\nPage number = \\(V \\gg 12\\) (right shift by 12 bits is equivalent to dividing by 4096)\nOffset = \\(V \\& (2^{12} - 1) = V \\& 0xFFF\\) (bit mask keeps the lower 12 bits)\nPage number 1 maps to frame number \\(f = F(1) = 9\\)\n\nTo compute the frame’s starting address, we use a left shift:\n\n\\(f \\ll 12 = 9 \\ll 12 = 36864\\), which is equivalent to \\(9 \\cdot 4096\\)\n\nFinal physical address:\n\n\\(\\text{Physical Address} = 36864 + 2904 = 39768\\)\n\nSame result, now using fast bit operations.\n\n\nBit Sequence Visualization\nLet’s visualize how the virtual address is split in binary:\n\nVirtual address \\(V = 7000\\)\nBinary representation (14 bits): 0001 1011 0101 1000\n\nSplit into:\n\nPage number (upper 2 bits): 00 01 → 1\nOffset (lower 12 bits): 1011 0101 1000 → 2904\n\nThis split works because:\n\nThe lower 12 bits represent the offset for a 4 KB page\nThe upper bits index into the page table\n\n\n\nWhy This Works Mathematically\nThe logic behind using bit shifts and masks instead of division and modulo is based on how numbers are represented in binary.\n\nDecimal Analogy (Base 10)\nConsider dividing 1375 by powers of 10:\n\n1375 ÷ 10¹ = 137 (modulo: 5)\n1375 ÷ 10² = 13 (modulo: 75)\n1375 ÷ 10³ = 1 (modulo: 375)\n\nThe rightmost digits are the remainder (modulo); the left are the quotient (division).\n\n\nBinary Example (Base 2)\nTake the binary number 1011₂ (= 11₁₀):\n\n1011 ÷ 2¹ = 101 = 5 (modulo: 1)\n1011 ÷ 2² = 10 = 2 (modulo: 11 = 3)\n1011 ÷ 2³ = 1 = 1 (modulo: 011 = 3)\n\nIn both systems, the rightmost digits/bits represent the offset, and the leftmost represent the page number.\nThis is why in binary:\n\n\\(V \\gg k\\) is equivalent to \\(\\left\\lfloor V / 2^k \\right\\rfloor\\)\n\\(V \\& (2^k - 1)\\) is equivalent to \\(V \\mod 2^k\\)\n\\(f \\ll k\\) is equivalent to \\(f \\cdot 2^k\\), which gives the frame base address\n\nThese operations are both mathematically correct and hardware-efficient.\n\n\n\nFinal Formula\n\\[\n\\text{Physical Address} = \\left( F(V \\gg k) \\ll k \\right) + \\left( V \\& (2^k - 1) \\right)\n\\]\nThis computes:\n\nThe page number via right shift\nThe frame number from the page table\nThe frame base via left shift (i.e., multiplying by page size)\nThe final physical address by adding the offset\n\n\n\nAdditional Example for Practice and Clarity\nLet’s now take another address and apply all three methods for reinforcement.\n\nSetup\n\nVirtual address \\(V = 13,\\!452\\)\nPage size = 4096 = \\(2^{12}\\)\nPage table:\n\n\n\n\nPage #\nFrame #\n\n\n\n\n0\n3\n\n\n1\n7\n\n\n2\n1\n\n\n3\n6\n\n\n\n\n\nManual Calculation\n\nPage number: \\(13,\\!452 \\div 4096 = 3\\)\nOffset: \\(13,\\!452 \\mod 4096 = 1164\\)\nFrame number: \\(F(3) = 6\\)\nPhysical address = \\(6 \\cdot 4096 + 1164 = 24,\\!576 + 1164 = 25,\\!740\\)\n\n\n\nBitwise Calculation\n\n\\(V = 13,\\!452 = 0b0011\\ 0100\\ 1001\\ 1100\\)\nPage number = \\(V \\gg 12 = 3\\)\nOffset = \\(V \\& 0xFFF = 1164\\)\nFrame number = \\(F(3) = 6\\)\nFrame base = \\(6 \\ll 12 = 24,\\!576\\)\nPhysical address = \\(24,\\!576 + 1164 = 25,\\!740\\)\n\n\n\n\nUsing the Formula\n\\[\n\\text{Physical Address} = (F(V \\gg 12) \\ll 12) + (V \\& 0xFFF)\n\\]\n\\[\n= (6 \\ll 12) + 1164 = 24,\\!576 + 1164 = 25,\\!740\n\\]\n\n\nConclusion\nWhen the page size is a power of two, address translation can be performed using fast bit operations instead of division and modulo. This is possible because of how binary numbers encode positional value. We saw that the lower bits give the offset and the upper bits the page number. Whether done manually, with bit operations, or using the translation formula, all approaches yield the same physical address — and this consistency is what makes paging both robust and efficient.\n\n\n\nSingle-Level (Direct) Page Tables\nIn the simplest form of paging, each process has its own single-level page table, which directly maps virtual page numbers to physical frame numbers.\nFor example, in a system with:\n\nA 32-bit virtual address space (4 GB total)\nA page size of 4 KB = 2¹² bytes\n\nThe number of virtual pages is:\n\\[\n2^{32} / 2^{12} = 2^{20} = 1,\\!048,\\!576 \\text{ entries}\n\\]\nIf each page table entry (PTE) is 4 bytes, the total size of the page table is:\n\\[\n2^{20} \\times 4 = 4 \\text{ MB} \\text{ per process}\n\\]\nIn a 64-bit system, even with larger pages (e.g. 4 MB), the number of virtual pages is so large (e.g., \\(2^{52}\\)) that flat page tables become completely impractical.\n\n\n\nWhy Single-Level Tables Are Impractical\nMain issues:\n\nMemory usage per process becomes excessive (e.g., 4 MB/page table × hundreds of processes)\nScaling issues as address spaces grow\nMost processes use only a small part of their virtual address space, so allocating full page tables is wasteful\n\nThus, alternative paging strategies are needed.\n\n\n\nFrame Table (Global Physical Memory Tracking)\nThe OS maintains a frame table, which tracks:\n\nWhich physical frames are in use\nWhat each frame is used for (user page, kernel structure, page table, etc.)\nAssociated metadata: dirty bit, reference count, owner process\n\nThis allows the OS to allocate and deallocate physical memory intelligently and safely, and is crucial for page replacement, memory protection, and I/O operations.\n\n\n\nInverted Page Tables\nInstead of one page table per process, an inverted page table contains one entry per physical frame. Each entry stores:\n\nProcess ID\nVirtual page number mapped to this frame\n\nThe index in the table corresponds to the physical frame number.\nBenefits:\n\nMemory usage depends only on RAM size, not virtual address space\n\nDrawbacks:\n\nTo resolve a virtual address, the system must search (or hash) the inverted table to find the matching (PID, VPN) pair\nMore complex lookup; slower than indexed access\nDifficult to implement efficient TLB caching\n\nUsed in some older or memory-constrained systems, but rare in modern general-purpose OSes.\n\n\n\nHierarchical Page Tables\nModern systems (e.g., x86, Linux, Windows) use multi-level page tables to address sparsity in virtual memory usage.\nThe virtual address is divided into parts:\n\nEach part indexes a level in the page table hierarchy\nOnly needed page table levels are allocated on demand\n\n\n\n32-bit Two-Level Paging Example\nFor a 32-bit address space with 4 KB pages:\n\\[\n\\text{Virtual address} = [10-bit Page Dir] | [10-bit Page Table] | [12-bit Offset]\n\\]\n\n1024 entries in the Page Directory (PD)\nEach PD entry points to a Page Table (PT) with 1024 entries\nFinal PT entry points to the physical frame\n\nOnly PD is allocated initially. PTs are created only when needed, saving memory.\n\n\n\n64-bit Four-Level Paging Example (x86-64)\nTypically uses 48-bit virtual addresses, split into:\n\\[\n[9-bit PML4] | [9-bit PDPT] | [9-bit PD] | [9-bit PT] | [12-bit offset]\n\\]\nEach level (PML4, PDPT, PD, PT) is 512 entries (9 bits).\nThis allows mapping of:\n\\[\n512^4 \\times 4\\text{ KB} = 256 \\text{ TB of virtual address space}\n\\]\nPage tables are allocated only as needed — i.e., when a virtual page is first accessed.\n\n\n\n\nWhen and How Page Tables Are Allocated\nPage tables are allocated in two situations:\n\nAt program load time\n\nWhen the OS loads a process, it allocates top-level tables and reserves virtual regions for code, data, stack, etc.\nNot all intermediate tables are allocated yet.\n\nOn demand via page faults\n\nWhen a process accesses a virtual address with no current mapping, the CPU triggers a page fault\nThe OS checks if the access is valid (e.g., within heap or stack)\nIf valid:\n\nMissing intermediate page tables are allocated\nA physical frame is allocated\nPage tables are updated\nExecution resumes\n\n\n\n\n\n\nPhysical Memory: Frames and Their Usage\nRAM is divided into fixed-size frames (e.g., 4 KB). Each frame can hold:\n\nA user page (code, stack, heap)\nA page table (of any level)\nA kernel structure\nAnything the OS needs\n\nImportant: contiguous physical frames may contain totally unrelated contents, from different processes or different kinds of structures. The OS manages this via the frame table.\n\n\n\nKernel Mapping and Access\n\nKernel code and data are mapped into the upper portion of each process’s virtual address space\n\nE.g., in 32-bit Linux: 0xC0000000 – 0xFFFFFFFF\n\nThis region is not accessible in user mode (protected by page table flags)\n\nBenefits:\n\nEnables fast system calls and interrupt handling without switching page tables\nAvoids need to reload CR3 (page table base register) on each syscall\n\nKernel runs entirely in kernel mode, and its physical location is:\n\nNot fixed\nDetermined at boot (by the bootloader)\nMapped into kernel virtual memory\n\nModern systems may use KASLR (Kernel Address Space Layout Randomization) to randomize the kernel’s location for security.\n\n\n\nTranslation Lookaside Buffer\nThe TLB is a small, fast cache used by the CPU’s MMU to store recent virtual-to-physical translations.\nWhy it’s needed:\n\nPage table lookups involve multiple memory accesses (especially in multi-level schemes)\nTLB provides near-instant translation for repeated accesses\n\nEffectiveness:\n\nVery high hit rate due to spatial and temporal locality\nTLBs are usually 16–512 entries\nManaged by the hardware; updated on TLB misses\n\n\n\n\nFinal Thoughts\nModern virtual memory management systems are built around the idea of:\n\nSparse usage of large address spaces\nOn-demand, hierarchical translation structures\nFast caching and protection mechanisms\n\nDirect page tables are conceptually simple but infeasible for large systems. Hierarchical page tables, supported by frame tables and TLBs, provide the scalability, flexibility, and efficiency required in today’s general-purpose operating systems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Memory Management</span>"
    ]
  }
]